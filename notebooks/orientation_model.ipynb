{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json \n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from torch import nn, optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crop the images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crop function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_out_background(images_dir, labels_dir, output_dir):\n",
    "    \"\"\"\n",
    "    Process all images in a directory and crop out the background using bounding boxes.\n",
    "\n",
    "    Parameters:\n",
    "        images_dir (str): Directory containing the images.\n",
    "        labels_dir (str): Directory containing the bounding box labels in text format.\n",
    "        output_dir (str): Directory to save the cropped images.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Loop through all image files in the images_dir\n",
    "    for image_path in Path(images_dir).glob(\"*.jpg\"):\n",
    "        # Get the corresponding label file\n",
    "        label_path = Path(labels_dir) / f\"{image_path.stem}.txt\"\n",
    "\n",
    "        # Check if the label file exists\n",
    "        if not label_path.exists():\n",
    "            print(f\"No label found for {image_path.name}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Load the image\n",
    "        image = cv2.imread(str(image_path))\n",
    "        if image is None:\n",
    "            print(f\"Could not read the image at {image_path}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Get image dimensions\n",
    "        img_height, img_width = image.shape[:2]\n",
    "\n",
    "        # Create a blank mask with the same dimensions as the image\n",
    "        mask = np.zeros(image.shape[:2], dtype=np.uint8)\n",
    "\n",
    "        # Read the bounding boxes from the label file\n",
    "        bounding_boxes = []\n",
    "        with open(label_path, 'r') as file:\n",
    "            for line in file:\n",
    "                class_id, x_center, y_center, width, height = map(float, line.strip().split())\n",
    "                bounding_boxes.append([x_center, y_center, width, height])\n",
    "\n",
    "        # Draw white rectangles for each bounding box on the mask\n",
    "        for (x_center, y_center, width, height) in bounding_boxes:\n",
    "            # Convert normalized coordinates to pixel values\n",
    "            x = int((x_center - width / 2) * img_width)\n",
    "            y = int((y_center - height / 2) * img_height)\n",
    "            w = int(width * img_width)\n",
    "            h = int(height * img_height)\n",
    "\n",
    "            # Draw the rectangle on the mask\n",
    "            cv2.rectangle(mask, (x, y), (x + w, y + h), 255, thickness=-1)\n",
    "\n",
    "        # Apply the mask to the image\n",
    "        result = cv2.bitwise_and(image, image, mask=mask)\n",
    "\n",
    "        # Save the result to the output directory\n",
    "        output_path = Path(output_dir) / image_path.name\n",
    "        cv2.imwrite(str(output_path), result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dir = '../data/with_labels/images'      # Directory with images\n",
    "labels_dir = '../data/processed/labels'      # Directory with bounding box labels (text files)\n",
    "output_dir = '../data/with_labels/images_cropped'     # Directory to save cropped images\n",
    "\n",
    "crop_out_background(images_dir, labels_dir, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flip orientation model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Class for Orientation Classification\n",
    "class OrientationDataset(Dataset):\n",
    "    def __init__(self, csv_file, images_dir, transform=None):\n",
    "        self.labels = pd.read_csv(csv_file)\n",
    "        self.images_dir = images_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.images_dir, self.labels.iloc[idx, 0])\n",
    "        label = 0 if self.labels.iloc[idx, 1] == \"left\" else 1  # 0 = left, 1 = right\n",
    "        image = Image.open(image_path)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function\n",
    "def train_orientation_classifier(train_csv, train_images_dir, num_epochs=10, checkpoint_path=\"orientation_classifier_checkpoint.pth\"):\n",
    "    # Dataset and DataLoader\n",
    "    train_dataset = OrientationDataset(train_csv, train_images_dir, transform=transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "    # Model\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    model.fc = nn.Linear(model.fc.in_features, 2)  # Binary classification\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Loss and Optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Training Loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss:.4f}\")\n",
    "\n",
    "    # Save Checkpoint\n",
    "    save_checkpoint(model, optimizer, num_epochs, path=checkpoint_path)\n",
    "    print(\"Training complete. Model checkpoint saved.\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# Save Checkpoint Function\n",
    "def save_checkpoint(model, optimizer, epoch, path):\n",
    "    checkpoint = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint, path)\n",
    "    print(f\"Checkpoint saved to {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths for Training Data (Kaggle paths)\n",
    "train_csv = \"data/manual_labeling/labels.csv\"\n",
    "train_images_dir = \"data/manual_labeling/images\"\n",
    "checkpoint_path = \"orientation_classifier_checkpoint.pth\"\n",
    "\n",
    "# Train the Model\n",
    "model = train_orientation_classifier(train_csv, train_images_dir, num_epochs=10, checkpoint_path=checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the checkpoint of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def load_checkpoint(path, model, optimizer=None):\n",
    "    \"\"\"\n",
    "    Load a model checkpoint and map it to the appropriate device (CPU or GPU).\n",
    "\n",
    "    Parameters:\n",
    "        path (str): Path to the checkpoint file.\n",
    "        model (torch.nn.Module): Model to load the weights into.\n",
    "        optimizer (torch.optim.Optimizer, optional): Optimizer to load the state.\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(path, map_location=device)  # Ensure mapping to the correct device\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    if optimizer:\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    print(f\"Checkpoint loaded from {path}\")\n",
    "    return checkpoint.get(\"epoch\", None)\n",
    "\n",
    "# Example: Loading the trained model\n",
    "model = models.resnet18(pretrained=False)\n",
    "model.fc = nn.Linear(model.fc.in_features, 2)  # Adjust for binary classification\n",
    "model = model.to(device)\n",
    "\n",
    "checkpoint_path = \"../checkpoints/orientation_classifier_checkpoint.pth\"\n",
    "load_checkpoint(checkpoint_path, model)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_flip_images_with_labels(model, images_dir, labels_dir, output_image_dir, output_label_dir, desired_orientation=\"left\"):\n",
    "    \"\"\"\n",
    "    Predict the orientation of images, flip them if necessary, and adjust the corresponding labels.\n",
    "\n",
    "    Parameters:\n",
    "        model (torch.nn.Module): Trained orientation model.\n",
    "        images_dir (str): Directory containing input images.\n",
    "        labels_dir (str): Directory containing corresponding JSON annotations (LabelMe format).\n",
    "        output_image_dir (str): Directory to save flipped images.\n",
    "        output_label_dir (str): Directory to save updated labels.\n",
    "        desired_orientation (str): Desired car orientation (\"left\" or \"right\").\n",
    "    \"\"\"\n",
    "    os.makedirs(output_image_dir, exist_ok=True)\n",
    "    os.makedirs(output_label_dir, exist_ok=True)\n",
    "    predictions = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for image_path in Path(images_dir).glob(\"*.jpg\"):\n",
    "            image = Image.open(image_path)\n",
    "            image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "            # Predict orientation\n",
    "            output = model(image_tensor)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            label = \"left\" if predicted.item() == 0 else \"right\"\n",
    "\n",
    "            # Flip image if needed\n",
    "            original_image = cv2.imread(str(image_path))\n",
    "            h, w = original_image.shape[:2]  # Get image dimensions\n",
    "            if label != desired_orientation:\n",
    "                flipped_image = cv2.flip(original_image, 1)  # Flip horizontally\n",
    "            else:\n",
    "                flipped_image = original_image\n",
    "\n",
    "            # Save flipped (or unchanged) image\n",
    "            output_image_path = Path(output_image_dir) / image_path.name\n",
    "            cv2.imwrite(str(output_image_path), flipped_image)\n",
    "\n",
    "            # Update labels if available\n",
    "            label_path = Path(labels_dir) / f\"{image_path.stem}.json\"\n",
    "            if label_path.exists():\n",
    "                with open(label_path, \"r\") as f:\n",
    "                    label_data = json.load(f)\n",
    "\n",
    "                if label != desired_orientation:\n",
    "                    # Flip label points horizontally\n",
    "                    for shape in label_data[\"shapes\"]:\n",
    "                        for point in shape[\"points\"]:\n",
    "                            point[0] = w - point[0]  # Adjust x-coordinate for flipping\n",
    "\n",
    "                # Save updated label\n",
    "                output_label_path = Path(output_label_dir) / label_path.name\n",
    "                with open(output_label_path, \"w\") as f:\n",
    "                    json.dump(label_data, f, indent=4)\n",
    "\n",
    "            print(f\"Processed: {image_path.name}\")\n",
    "\n",
    "            # Save prediction for reference\n",
    "            predictions.append({\"image\": image_path.name, \"label\": label})\n",
    "\n",
    "    # Save predictions to CSV\n",
    "    pd.DataFrame(predictions).to_csv(Path(output_image_dir) / \"predictions.csv\", index=False)\n",
    "    print(f\"Predictions, flipped images, and updated labels saved in {output_image_dir} and {output_label_dir}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_orientation = \"left\"\n",
    "\n",
    "# Directory containing all JSON labels\n",
    "labels_dir = \"../data/with_labels/json\" \n",
    "\n",
    "'''\n",
    "# Training images\n",
    "training_images_dir = \"../data/split/train/images\"\n",
    "training_flipped_output_dir = \"../data/split/train/images_flipped\"\n",
    "training_labels_flipped_output_dir = \"../data/split/train/labels_flipped\"\n",
    "predict_and_flip_images_with_labels(\n",
    "    model,\n",
    "    training_images_dir,\n",
    "    labels_dir,\n",
    "    training_flipped_output_dir,\n",
    "    training_labels_flipped_output_dir,\n",
    "    desired_orientation\n",
    ")\n",
    "\n",
    "# Validation images\n",
    "validation_images_dir = \"../data/split/val/images\"\n",
    "validation_flipped_output_dir = \"../data/split/val/images_flipped\"\n",
    "validation_labels_flipped_output_dir = \"../data/split/val/labels_flipped\"\n",
    "predict_and_flip_images_with_labels(\n",
    "    model,\n",
    "    validation_images_dir,\n",
    "    labels_dir,\n",
    "    validation_flipped_output_dir,\n",
    "    validation_labels_flipped_output_dir,\n",
    "    desired_orientation\n",
    ")\n",
    "\n",
    "# Test images\n",
    "test_images_dir = \"../data/split/test/images\"\n",
    "test_flipped_output_dir = \"../data/split/test/images_flipped\"\n",
    "test_labels_flipped_output_dir = \"../data/split/test/labels_flipped\"\n",
    "predict_and_flip_images_with_labels(\n",
    "    model,\n",
    "    test_images_dir,\n",
    "    labels_dir,\n",
    "    test_flipped_output_dir,\n",
    "    test_labels_flipped_output_dir,\n",
    "    desired_orientation\n",
    ")\n",
    "'''\n",
    "\n",
    "images_dir = \"../data/with_labels/images_cropped\"\n",
    "output_image_dir = \"../data/with_labels/images_flipped\"\n",
    "output_label_dir = \"../data/with_labels/json_flipped\"\n",
    "predict_and_flip_images_with_labels(\n",
    "    model,\n",
    "    images_dir,\n",
    "    labels_dir,\n",
    "    output_image_dir,\n",
    "    output_label_dir,\n",
    "    desired_orientation\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
