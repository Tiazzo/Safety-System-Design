{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess\n",
    "This notebook is responsible for preprocessing the images. Run this notebook if you want to train the models.\n",
    "\n",
    "## Contents:\n",
    "**0. Configuration**\n",
    "- Configuring paths\n",
    "- Unzipping the GP22 dataset\n",
    "- Verifying paths\n",
    "\n",
    "**1. Preprocessing** (will be performed on the images and labels inside the `data/model_training` folder)\n",
    "- Crop (Remove background noise)\n",
    "- Flip (Every car should be in the same direction)\n",
    "- Scale (Scale all cars according to a reference car)\n",
    "- Augmentation (Data augmentation on the images)\n",
    "- Execute (Run the scripts)\n",
    "\n",
    "**2. Visualization**\n",
    "- Here you can visualize the preprocessing steps and verify their integrity\n",
    "\n",
    "**3. Splitting**\n",
    "- The dataset will be split into train, test, and validation sets\n",
    "- Also includes visualization of the final dataset\n",
    "\n",
    "What will you need to do?\n",
    "- Create a dir named *data* in the root folder of the project\n",
    "- Download the GP22 Dataset (https://zenodo.org/records/6366808), both Images.zip and Labels.zip\n",
    "- Put Images.zip and Labels.zip in the data folder\n",
    "- Run code block below to generate directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: These preprocessing steps are performed on the images contained in the model_training folder (can be found on [Teams](https://chalmers.sharepoint.com/:f:/s/StudentTRA235/EucRJsbp-FVHtJNnHZEIZhIB0xyTaJHSPOQlXy4_4Zfp0g?e=JjSBZN)). These paths refer to previously annotated data, including images and their associated JSON labels.\n",
    "If you want to add more data, you can manually annotate new images using LabelMe and place both the images and their corresponding JSON labels into the following folders: data/model_training/images and data/model_training/json_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure paths here \n",
    "path_to_orientation_model = \"../checkpoints/orientation_classifier_checkpoint.pth\"\n",
    "path_to_gp22_images = \"../data/GP22/images\"\n",
    "path_to_gp22_labels = \"../data/GP22/labels\"\n",
    "path_images_for_training = \"../data/model_training/images\"\n",
    "path_to_txt_labels_for_training = \"../data/model_training/labels_txt\"\n",
    "path_to_json_labels_for_training = \"../data/model_training/json_labels\"\n",
    "\n",
    "\n",
    "output_dir_images_cropped = \"../data/processed/cropped/images\"\n",
    "output_dir_labels_cropped = \"../data/processed/cropped/labels\"\n",
    "output_dir_images_flipped = \"../data/processed/flipped/images\"\n",
    "output_dir_labels_txt_flipped = \"../data/processed/flipped/txt_labels\"\n",
    "output_dir_labels_json_flipped = \"../data/processed/flipped/json_labels\"\n",
    "output_dir_scaled = \"../data/processed/scaled\"\n",
    "output_dir_images_scaled = \"../data/processed/scaled/images\"\n",
    "output_augmentations_dir = \"../data/processed/augmentations\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import torch\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import json\n",
    "import pandas as pd\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 Directories generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ensure_directories_exist(paths):\n",
    "    \"\"\" \n",
    "    Ensure that the directories in the given paths exist. If they do not, create them.\n",
    "    Args:\n",
    "        paths (list): List of paths to directories to ensure exist.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    for path in paths:\n",
    "        directory = path if path.endswith('/') or '.' not in os.path.basename(path) else os.path.dirname(path)\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "            print(f\"Created directory: {directory}\")\n",
    "        else:\n",
    "            print(f\"Directory already exists: {directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure paths here\n",
    "paths = [\n",
    "    \"../data\",\n",
    "    \"../data/GP22/images\",\n",
    "    \"../data/GP22/labels\",\n",
    "    \"../data/model_training/images\",\n",
    "    \"../data/model_training/json_labels\",\n",
    "    \"../data/model_training/labels_txt\",\n",
    "    \"../data/processed/cropped/images\",\n",
    "    \"../data/processed/cropped/labels\",\n",
    "    \"../data/processed/flipped/images\",\n",
    "    \"../data/processed/flipped/txt_labels\",\n",
    "    \"../data/processed/flipped/json_labels\",\n",
    "    \"../data/processed/scaled\",\n",
    "    \"../data/processed/augmentations/images\",\n",
    "    \"../data/processed/augmentations/masks\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensure_directories_exist(paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.4 Unzip of GP22 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "base_dir = \"../data/GP22\"\n",
    "images_zip = \"../data/Images.zip\"\n",
    "labels_zip = \"../data/Labels.zip\"\n",
    "images_dir = os.path.join(base_dir, \"images\")\n",
    "labels_dir = os.path.join(base_dir, \"labels\")\n",
    "\n",
    "# Function to remove __MACOSX folder if it exists\n",
    "def remove_macosx_folder(base_dir):\n",
    "    macosx_path = os.path.join(base_dir, \"__MACOSX\")\n",
    "    if os.path.exists(macosx_path):\n",
    "        shutil.rmtree(macosx_path)\n",
    "        print(f\"Removed {macosx_path}.\")\n",
    "\n",
    "# Check and unzip Images.zip\n",
    "if os.path.exists(images_zip):\n",
    "    if not os.listdir(images_dir):\n",
    "        print(f\"Extracting {images_zip} to {images_dir}...\")\n",
    "        with zipfile.ZipFile(images_zip, 'r') as zip_ref:\n",
    "            zip_ref.extractall(base_dir) \n",
    "        print(f\"Extraction complete: {images_dir}\")\n",
    "        os.remove(images_zip)\n",
    "        print(f\"Deleted {images_zip}.\")\n",
    "        remove_macosx_folder(base_dir)\n",
    "    else:\n",
    "        print(f\"{images_dir} already contains files. Skipping extraction of {images_zip}.\")\n",
    "else:\n",
    "    print(f\"{images_zip} not found.\")\n",
    "\n",
    "# Check and unzip Labels.zip\n",
    "if os.path.exists(labels_zip):\n",
    "    if not os.listdir(labels_dir):\n",
    "        print(f\"Extracting {labels_zip} to {labels_dir}...\")\n",
    "        with zipfile.ZipFile(labels_zip, 'r') as zip_ref:\n",
    "            zip_ref.extractall(base_dir)\n",
    "        print(f\"Extraction complete: {labels_dir}\")\n",
    "        os.remove(labels_zip)\n",
    "        print(f\"Deleted {labels_zip}.\")\n",
    "        remove_macosx_folder(base_dir)\n",
    "    else:\n",
    "        print(f\"{labels_dir} already contains files. Skipping extraction of {labels_zip}.\")\n",
    "else:\n",
    "    print(f\"{labels_zip} not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if each image has a corresponding label\n",
    "def check_images_labels(images_path, labels_path):\n",
    "    images = os.listdir(images_path)\n",
    "    labels = os.listdir(labels_path)\n",
    "    images = {\n",
    "        os.path.splitext(f)[0]\n",
    "        for f in os.listdir(images_path)\n",
    "        if f.endswith(\".jpg\") and not f.startswith(\".\")\n",
    "    }\n",
    "    labels = {\n",
    "        os.path.splitext(f)[0]\n",
    "        for f in os.listdir(labels_path)\n",
    "        # if f.endswith(\".json\") and not f.startswith(\".\") # for json labels\n",
    "        if f.endswith(\".txt\") and not f.startswith(\".\") # for txt labels\n",
    "    }\n",
    "    if set(images) == set(labels):\n",
    "        print(\"All images have corresponding labels\")\n",
    "    else:\n",
    "        print(\"Some images don't have corresponding labels\")\n",
    "        print(\"Images without labels: \", set(images) - set(labels))\n",
    "        print(\"Labels without images: \", set(labels) - set(images))\n",
    "\n",
    "# Change the paths\n",
    "check_images_labels(path_to_gp22_images, path_to_gp22_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Crop\n",
    "Removing background noise improves model performance.\n",
    "The following code block will: \n",
    "- Use GP22 labels to process the images and remove background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_out_background(images_dir, labels_dir, output_images_dir, output_labels_dir):\n",
    "    \"\"\"\n",
    "    Process all images in a directory, crop out the background using bounding boxes, \n",
    "    and save updated labels to a new directory.\n",
    "\n",
    "    Args:\n",
    "        images_dir (str): Path to the directory containing the images.\n",
    "        labels_dir (str): Path to the directory containing the labels.\n",
    "        output_images_dir (str): Path to the directory to save the cropped images.\n",
    "        output_labels_dir (str): Path to the directory to save the updated labels.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    os.makedirs(output_images_dir, exist_ok=True)\n",
    "    os.makedirs(output_labels_dir, exist_ok=True)\n",
    "\n",
    "    for image_path in Path(images_dir).glob(\"*.jpg\"):\n",
    "        label_path = Path(labels_dir) / f\"{image_path.stem}.txt\"\n",
    "\n",
    "        if not label_path.exists():\n",
    "            print(f\"No label found for {image_path.name}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        image = cv2.imread(str(image_path))\n",
    "        if image is None:\n",
    "            print(f\"Could not read the image at {image_path}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        img_height, img_width = image.shape[:2]\n",
    "\n",
    "        mask = np.zeros(image.shape[:2], dtype=np.uint8)\n",
    "\n",
    "        bounding_boxes = []\n",
    "        updated_labels = []\n",
    "        with open(label_path, 'r') as file:\n",
    "            for line in file:\n",
    "                class_id, x_center, y_center, width, height = map(float, line.strip().split())\n",
    "                bounding_boxes.append([x_center, y_center, width, height])\n",
    "\n",
    "        for (x_center, y_center, width, height) in bounding_boxes:\n",
    "            x = int((x_center - width / 2) * img_width)\n",
    "            y = int((y_center - height / 2) * img_height)\n",
    "            w = int(width * img_width)\n",
    "            h = int(height * img_height)\n",
    "\n",
    "            # Draw bounding box on the mask\n",
    "            cv2.rectangle(mask, (x, y), (x + w, y + h), 255, thickness=-1)\n",
    "\n",
    "            # Update labels with pixel coordinates\n",
    "            new_x_center = (x + w / 2) / img_width\n",
    "            new_y_center = (y + h / 2) / img_height\n",
    "            new_width = w / img_width\n",
    "            new_height = h / img_height\n",
    "            updated_labels.append(f\"{int(class_id)} {new_x_center:.6f} {new_y_center:.6f} {new_width:.6f} {new_height:.6f}\")\n",
    "\n",
    "        # Apply the mask to the image\n",
    "        result = cv2.bitwise_and(image, image, mask=mask)\n",
    "\n",
    "        # Save the cropped image\n",
    "        output_image_path = Path(output_images_dir) / image_path.name\n",
    "        cv2.imwrite(str(output_image_path), result)\n",
    "\n",
    "        # Save the updated labels\n",
    "        output_label_path = Path(output_labels_dir) / f\"{image_path.stem}.txt\"\n",
    "        with open(output_label_path, 'w') as file:\n",
    "            file.write(\"\\n\".join(updated_labels))\n",
    "\n",
    "        print(f\"Processed and saved: {image_path.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Flip\n",
    "Every car should be pointing in the same direction.\n",
    "The following code blocks will:\n",
    "- Load the orientation model\n",
    "- Predict whether a car is pointing left or right\n",
    "- Flip the corresponding label of each car\n",
    "- Flip the image of the car itself\n",
    "\n",
    "We have chosen to flip cars to the **left**. That is, nose points to the left."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Load the orientation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_orientation_model(checkpoint_path):\n",
    "    \"\"\"\n",
    "    Loads the trained orientation model from the given checkpoint path.\n",
    "    Args:\n",
    "        checkpoint_path (str): Path to the checkpoint file.\n",
    "    Returns:\n",
    "        torch.nn.Module: The loaded model.\n",
    "    \"\"\"\n",
    "    model = models.resnet18(pretrained=False)\n",
    "    model.fc = torch.nn.Linear(model.fc.in_features, 2)\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    print(f\"Model loaded from {checkpoint_path}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Predict orientation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_orientation(model, image_path):\n",
    "    \"\"\"\n",
    "    Predicts the orientation of the car in the image.\n",
    "    Args:\n",
    "        model (torch.nn.Module): The orientation model.\n",
    "        image_path (str): Path to the image.\n",
    "    Returns:\n",
    "        str: The predicted orientation (\"left\" or \"right\").\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(image_tensor)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        return \"left\" if predicted.item() == 0 else \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Flip x-coordinates of txt labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_labels_txt_x(label_path, output_label_dir):\n",
    "    \"\"\"\n",
    "    Flips the x-coordinates of labels and saves them to a new directory.\n",
    "\n",
    "    Args:\n",
    "        label_path (str): Path to the label file.\n",
    "        output_label_dir (str): Path to the directory to save the flipped labels.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    os.makedirs(output_label_dir, exist_ok=True)\n",
    "\n",
    "    flipped_labels = []\n",
    "    with open(label_path, 'r') as f:\n",
    "        for line in f:\n",
    "            class_id, x_center, y_center, width, height = map(float, line.strip().split())\n",
    "            x_center = 1 - x_center  # Flip the x-coordinate\n",
    "            flipped_labels.append(f\"{int(class_id)} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\")\n",
    "\n",
    "    output_label_path = os.path.join(output_label_dir, os.path.basename(label_path))\n",
    "    with open(output_label_path, 'w') as f:\n",
    "        f.write(\"\\n\".join(flipped_labels))\n",
    "    \n",
    "    print(f\"Flipped labels saved to {output_label_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4 Flip x-coordinates of json labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_labels_json_x(label_path, output_label_dir, w):\n",
    "    \"\"\"\n",
    "    Flips the x-coordinates of labels in a JSON format and saves them.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_label_dir, exist_ok=True)\n",
    "    label_path = Path(label_path)\n",
    "    if label_path.exists():\n",
    "        with open(label_path, \"r\") as f:\n",
    "            label_data = json.load(f)\n",
    "            # Flip label points horizontally\n",
    "            for shape in label_data[\"shapes\"]:\n",
    "                for point in shape[\"points\"]:\n",
    "                    point[0] = w - point[0]  # Adjust x-coordinate for flipping\n",
    "\n",
    "        # Save updated label\n",
    "        output_label_path = Path(output_label_dir) / label_path.name\n",
    "        with open(output_label_path, \"w\") as f:\n",
    "            json.dump(label_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.5 Flip images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_image(image_path, output_image_dir):\n",
    "    \"\"\"\n",
    "    Flips an image horizontally and saves it to the output directory.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the image.\n",
    "        output_image_dir (str): Path to the directory to save the flipped image.\n",
    "    Returns:\n",
    "        str: Path to the saved flipped image.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_image_dir, exist_ok=True)\n",
    "\n",
    "    image = cv2.imread(str(image_path))\n",
    "    h, w = image.shape[:2]  # Get image dimensions\n",
    "\n",
    "    flipped_image = cv2.flip(image, 1)\n",
    "    output_image_path = os.path.join(output_image_dir, os.path.basename(image_path))\n",
    "    cv2.imwrite(output_image_path, flipped_image)\n",
    "    \n",
    "    print(f\"Flipped image saved to {output_image_path}\")\n",
    "    return output_image_path, w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.6 Flip Images and Labels\n",
    "Run this to process all images and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_images_and_labels(model, images_dir, labels_txt_dir, labels_json_dir, output_image_dir, output_txt_label_dir, output_json_label_dir):\n",
    "    \"\"\"\n",
    "    Flips images and labels horizontally based on the predicted orientation.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The orientation model.\n",
    "        images_dir (str): Path to the directory containing the images.\n",
    "        labels_txt_dir (str): Path to the directory containing the txt labels.\n",
    "        labels_json_dir (str): Path to the directory containing the json labels.\n",
    "        output_image_dir (str): Path to the directory to save the flipped images.\n",
    "        output_txt_label_dir (str): Path to the directory to save the flipped txt labels.\n",
    "        output_json_label_dir (str): Path to the directory to save the flipped json labels.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_image_dir, exist_ok=True)\n",
    "    os.makedirs(output_txt_label_dir, exist_ok=True)\n",
    "    os.makedirs(output_json_label_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "    for image_path in Path(images_dir).glob(\"*.jpg\"):\n",
    "        label_txt_path = Path(labels_txt_dir) / f\"{image_path.stem}.txt\"\n",
    "        label_json_path = Path(labels_json_dir) / f\"{image_path.stem}.json\"\n",
    "        \n",
    "        if label_txt_path.exists() and label_json_path.exists():\n",
    "            orientation = predict_orientation(model, str(image_path))\n",
    "            flipped = False\n",
    "\n",
    "            if orientation == \"right\":\n",
    "                flipped = True\n",
    "                _, w = flip_image(str(image_path), output_image_dir)\n",
    "                flip_labels_txt_x(str(label_txt_path), output_txt_label_dir)\n",
    "                flip_labels_json_x(str(label_json_path), output_json_label_dir, w)\n",
    "            else:\n",
    "                shutil.copy(str(image_path), os.path.join(output_image_dir, image_path.name))\n",
    "                shutil.copy(str(label_txt_path), os.path.join(output_txt_label_dir, label_txt_path.name))\n",
    "                shutil.copy(str(label_json_path), os.path.join(output_json_label_dir, label_json_path.name))\n",
    "                print(f\"Image and labels copied to {output_image_dir} and {output_txt_label_dir}\" and {output_json_label_dir})\n",
    "        else:\n",
    "            print(f\"Label file not found for {image_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Scale\n",
    "Cars should be relative to each other in size.\n",
    "The following code blocks will:\n",
    "- Calculate rim area (used as a reference when scaling)\n",
    "- Calculate scaling factor (based on reference car and current car)\n",
    "- Scale images and update labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 Rim area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rim_area_of_front_wheel(label_path, resolution=1024):\n",
    "    \"\"\"\n",
    "    Returns {area} of front-wheel rim\n",
    "    Important! Car nose should be pointing left\n",
    "\n",
    "    Args:\n",
    "        label_path (str): Path to the label file.\n",
    "        resolution (int): Resolution of the image.\n",
    "    Returns:\n",
    "        float: Area of the front-wheel rim in the image.\n",
    "    \"\"\"\n",
    "    smallest_x = float(\"inf\")\n",
    "    smallest_box = None\n",
    "\n",
    "    with open(label_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            class_id, x_center, y_center, width, height = line.strip().split()\n",
    "            if int(class_id) == 1: \n",
    "                x_center = float(x_center)\n",
    "                if x_center < smallest_x:\n",
    "                    smallest_x = x_center\n",
    "                    smallest_box = (float(width), float(height))\n",
    "\n",
    "    if smallest_box:\n",
    "        width, height = smallest_box\n",
    "        area = width * height * (resolution**2)\n",
    "        return area\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 Scaling factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_area(rim_area_reference_car, rim_area_current_car):\n",
    "    \"\"\"\n",
    "    Calculate the scaling factor to adjust the dimensions of the current car\n",
    "    so that its area matches the area of the reference car.\n",
    "\n",
    "    Args:\n",
    "        rim_area_reference_car (float): Area of the front-wheel rim of the reference car.\n",
    "        rim_area_current_car (float): Area of the front-wheel rim of the current car.\n",
    "    Returns:\n",
    "        float: Scaling factor to adjust the dimensions of the current car.\n",
    "    \"\"\"\n",
    "    if rim_area_reference_car <= 0 or rim_area_current_car <= 0:\n",
    "        raise ValueError(\"Both areas must be positive numbers.\")\n",
    "    \n",
    "    scaling_factor = (rim_area_reference_car / rim_area_current_car) ** 0.5\n",
    "    \n",
    "    return scaling_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3 Scale JSON labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "def update_json_labels(json_path, scaling_factor, offset_x, offset_y, labels_dir, padded_image_path):\n",
    "    \"\"\"\n",
    "    Updates the JSON labels to match the scaled and padded image and updates the base64 string.\n",
    "\n",
    "    Args:\n",
    "        json_path (str): Path to the JSON label file.\n",
    "        scaling_factor (float): Scaling factor to adjust the dimensions of the current car.\n",
    "        offset_x (int): The x-offset to center the points.\n",
    "        offset_y (int): The y-offset to center the points.\n",
    "        labels_dir (str): Path to the directory to save the updated JSON labels.\n",
    "        padded_image_path (str): Path to the scaled and padded image.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    with open(json_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Scale and offset points\n",
    "    for shape in data['shapes']:\n",
    "        new_points = []\n",
    "        for point in shape['points']:\n",
    "            # First scale the points\n",
    "            scaled_x = point[0] * scaling_factor\n",
    "            scaled_y = point[1] * scaling_factor\n",
    "            \n",
    "            # Then add the positive offset to center the points\n",
    "            scaled_x += offset_x\n",
    "            scaled_y += offset_y\n",
    "\n",
    "            # Only include points that fall within the 1024x1024 canvas\n",
    "            if 0 <= scaled_x < 1024 and 0 <= scaled_y < 1024:\n",
    "                new_points.append([scaled_x, scaled_y])\n",
    "\n",
    "        shape['points'] = new_points\n",
    "\n",
    "    # Update image metadata\n",
    "    data['imageWidth'] = 1024\n",
    "    data['imageHeight'] = 1024\n",
    "\n",
    "    # Update the base64 string\n",
    "    with open(padded_image_path, 'rb') as img_file:\n",
    "        img_data = img_file.read()\n",
    "        encoded_data = base64.b64encode(img_data).decode('utf-8')\n",
    "        data['imageData'] = encoded_data\n",
    "\n",
    "    # Save updated JSON\n",
    "    json_filename = os.path.basename(json_path)\n",
    "    updated_json_path = os.path.join(labels_dir, json_filename)\n",
    "\n",
    "    with open(updated_json_path, 'w') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "    print(f\"Updated JSON labels and base64 for {json_filename}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.4 Scale images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_and_crop_image(image_path, scaling_factor, label_path, json_label_path, output_dir):\n",
    "    \"\"\"\n",
    "    Scales an image by a given scaling factor, ensures it is padded to 1024x1024 pixels,\n",
    "    and scales corresponding JSON labels.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the image.\n",
    "        scaling_factor (float): Scaling factor to adjust the dimensions of the current car.\n",
    "        label_path (str): Path to the label file.\n",
    "        json_label_path (str): Path to the JSON label file.\n",
    "        output_dir (str): Path to the directory to save the scaled image.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if scaling_factor <= 0:\n",
    "        raise ValueError(\"Scaling factor must be a positive number.\")\n",
    "\n",
    "    # Read the image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        raise FileNotFoundError(f\"Image file not found: {image_path}\")\n",
    "\n",
    "    # Get original dimensions\n",
    "    original_height, original_width = image.shape[:2]\n",
    "\n",
    "    # Compute new dimensions\n",
    "    new_width = int(original_width * scaling_factor)\n",
    "    new_height = int(original_height * scaling_factor)\n",
    "\n",
    "    # Resize the image using OpenCV\n",
    "    scaled_image = cv2.resize(\n",
    "        image, (new_width, new_height), interpolation=cv2.INTER_LINEAR\n",
    "    )\n",
    "\n",
    "    # Ensure the image is 1024x1024 by padding with black if necessary\n",
    "    target_size = 1024\n",
    "    padded_image = np.zeros((target_size, target_size, 3), dtype=np.uint8)\n",
    "\n",
    "    # Center the scaled image in the 1024x1024 canvas\n",
    "    offset_x = (target_size - new_width) // 2 if new_width < target_size else 0\n",
    "    offset_y = (target_size - new_height) // 2 if new_height < target_size else 0\n",
    "\n",
    "    insert_width = min(new_width, target_size)\n",
    "    insert_height = min(new_height, target_size)\n",
    "\n",
    "    padded_image[offset_y:offset_y+insert_height, offset_x:offset_x+insert_width] = \\\n",
    "        scaled_image[:insert_height, :insert_width]\n",
    "\n",
    "    # Create directories for images and labels\n",
    "    images_dir = os.path.join(output_dir, \"images\")\n",
    "    labels_dir = os.path.join(output_dir, \"labels\")\n",
    "    os.makedirs(images_dir, exist_ok=True)\n",
    "    os.makedirs(labels_dir, exist_ok=True)\n",
    "\n",
    "    # Save padded image\n",
    "    image_filename = os.path.basename(image_path)\n",
    "    padded_image_path = os.path.join(images_dir, image_filename)\n",
    "    cv2.imwrite(padded_image_path, padded_image)\n",
    "\n",
    "    # Update JSON labels\n",
    "    update_json_labels(json_label_path, scaling_factor, offset_x, offset_y, labels_dir, padded_image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_dataset(images_dir, masks_dir, output_dir, num_augmented=2):\n",
    "    \"\"\"\n",
    "    Augments an entire dataset of images and corresponding masks for segmentation tasks.\n",
    "\n",
    "    Parameters:\n",
    "        images_dir (str): Directory containing input images.\n",
    "        masks_dir (str): Directory containing input masks.\n",
    "        output_dir (str): Directory to save augmented images and masks.\n",
    "        num_augmented (int): Number of augmented copies to generate for each image-mask pair.\n",
    "    \"\"\"\n",
    "    transform = A.Compose(\n",
    "        [\n",
    "            A.RandomBrightnessContrast(p=0.2),  # Adjust brightness and contrast of the image.\n",
    "            A.GaussianBlur(blur_limit=(3, 5), p=0.2),  # Adds slight blurring.\n",
    "            A.HueSaturationValue(\n",
    "                hue_shift_limit=10, sat_shift_limit=20, val_shift_limit=10, p=0.3\n",
    "            ),  # Changes hue, saturation, and value.\n",
    "            A.Resize(1024, 1024, always_apply=True),  # Ensures image and mask are resized consistently.\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Create output directories\n",
    "    output_images_dir = Path(output_dir) / \"images\"\n",
    "    output_masks_dir = Path(output_dir) / \"masks\"\n",
    "    output_images_dir.mkdir(parents=True, exist_ok=True)\n",
    "    output_masks_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Process all images and masks\n",
    "    for image_path in Path(images_dir).glob(\"*.jpg\"):\n",
    "        mask_path = Path(masks_dir) / f\"{image_path.stem}.png\"\n",
    "        if not mask_path.exists():\n",
    "            print(f\"Mask for {image_path.name} not found. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Load image and mask\n",
    "        image = cv2.imread(str(image_path))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        for i in range(num_augmented):\n",
    "            # Apply augmentation\n",
    "            transformed = transform(image=image, mask=mask)\n",
    "            aug_image = transformed[\"image\"]\n",
    "            aug_mask = transformed[\"mask\"]\n",
    "\n",
    "            # Save augmented image and mask\n",
    "            aug_image_name = f\"{image_path.stem}_aug_{i}.jpg\"\n",
    "            aug_mask_name = f\"{mask_path.stem}_aug_{i}.png\"\n",
    "            cv2.imwrite(\n",
    "                str(output_images_dir / aug_image_name),\n",
    "                cv2.cvtColor(aug_image, cv2.COLOR_RGB2BGR),\n",
    "            )\n",
    "            cv2.imwrite(str(output_masks_dir / aug_mask_name), aug_mask)\n",
    "\n",
    "    print(f\"Augmented dataset saved in {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Execute\n",
    "The following code blocks will execute corresponding code for:\n",
    "- Removing background\n",
    "- Flipping images and labels\n",
    "- Scaling images and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.1 Crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_images_for_training          # Directory with GP22 images\n",
    "path_to_txt_labels_for_training          # Directory with bounding box labels (text files)\n",
    "output_dir_images_cropped    # Directory to save cropped images\n",
    "output_dir_labels_cropped    # Directory to save cropped labels\n",
    "\n",
    "crop_out_background(path_images_for_training, path_to_txt_labels_for_training, output_dir_images_cropped, output_dir_labels_cropped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.2 Flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_orientation_model   # Directory with orientation model\n",
    "output_dir_images_cropped   # Directory with cropped images\n",
    "path_to_json_labels_for_training         # Directory with JSON labels\n",
    "output_dir_images_flipped   # Directory to save flipped images\n",
    "output_dir_labels_txt_flipped  # Directory to save flipped labels txt\n",
    "output_dir_labels_json_flipped  # Directory to save flipped labels json\n",
    "\n",
    "model = load_orientation_model(path_to_orientation_model)\n",
    "\n",
    "flip_images_and_labels(\n",
    "    model, \n",
    "    output_dir_images_cropped,\n",
    "    output_dir_labels_cropped,\n",
    "    path_to_json_labels_for_training, \n",
    "    output_dir_images_flipped, \n",
    "    output_dir_labels_txt_flipped,\n",
    "    output_dir_labels_json_flipped\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.3 Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_all_images_and_labels(reference_label_path, images_folder, labels_folder, json_labels_folder, output_folder, resolution=1024):\n",
    "    \"\"\"\n",
    "    Scales images and corresponding labels\n",
    "    \"\"\"\n",
    "    # Create output directories\n",
    "    images_output_dir = os.path.join(output_folder, \"images\")\n",
    "    labels_output_dir = os.path.join(output_folder, \"labels\")\n",
    "    os.makedirs(images_output_dir, exist_ok=True)\n",
    "    os.makedirs(labels_output_dir, exist_ok=True)\n",
    "\n",
    "    # Ensure the reference label path is valid\n",
    "    if not os.path.exists(reference_label_path):\n",
    "        print(f\"Reference label path {reference_label_path} does not exist.\")\n",
    "        return\n",
    "    \n",
    "    reference_area = calculate_rim_area_of_front_wheel(reference_label_path, resolution)\n",
    "    print(f\"Reference area: {reference_area}\")\n",
    "    \n",
    "    # Loop through label files in the label folder\n",
    "    print(f\"Looking for label files in: {labels_folder}\")\n",
    "    for label_file in os.listdir(labels_folder):\n",
    "        if label_file.endswith(\".txt\"):\n",
    "            print(f\"Found label file: {label_file}\")\n",
    "            label_path = os.path.join(labels_folder, label_file)\n",
    "            \n",
    "            current_area = calculate_rim_area_of_front_wheel(label_path, resolution)\n",
    "            print(f\"Current area for {label_file}: {current_area}\")\n",
    "            \n",
    "            # Get corresponding image file\n",
    "            image_name = label_file.replace(\".txt\", \".jpg\")  # Assuming images are .jpg\n",
    "            image_path = Path(images_folder) / image_name  # Use Path to handle path joining\n",
    "            image_path = str(image_path).replace(\"\\\\\", \"/\")\n",
    "            print(f\"Looking for image: {image_path}\")\n",
    "            \n",
    "            if not os.path.exists(image_path):\n",
    "                print(f\"Image {image_name} corresponding to label {label_file} does not exist. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Get corresponding JSON label file (assumes the JSON label has the same name as the .txt label)\n",
    "            json_label_name = label_file.replace(\".txt\", \".json\")\n",
    "            json_label_path = os.path.join(json_labels_folder, json_label_name)\n",
    "            print(f\"Looking for JSON label: {json_label_path}\")\n",
    "            \n",
    "            if not os.path.exists(json_label_path):\n",
    "                print(f\"JSON label {json_label_name} for {label_file} does not exist. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            print(f\"Processing: {image_name} and {label_file}\")\n",
    "\n",
    "            scaling_factor = compare_area(reference_area, current_area)\n",
    "            print(f\"Scaling factor for {label_file}: {scaling_factor}\")\n",
    "\n",
    "            # Process the image and label\n",
    "            try:\n",
    "                scale_and_crop_image(image_path, scaling_factor, label_path, json_label_path, output_folder)\n",
    "                print(f\"Processed and saved: {image_name} and {label_file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {image_name} and {label_file}: {e}\")\n",
    "\n",
    "\n",
    "path_to_reference_car_label = \"../data/GP22/labels/B_Ren_12.txt\" # Directory to chosen reference car\n",
    "output_dir_images_flipped   # Directory to images of flipped cars\n",
    "output_dir_labels_json_flipped   # Directory to labels of flipped cars in json format\n",
    "output_dir_scaled           # Directory to save images and labels of scaled cars\n",
    "\n",
    "scale_all_images_and_labels(\n",
    "    path_to_reference_car_label, \n",
    "    output_dir_images_flipped, \n",
    "    path_to_gp22_labels, \n",
    "    output_dir_labels_json_flipped, \n",
    "    output_dir_scaled\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.4 Augmentation execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def create_masks(labelme_dir, output_mask_dir):\n",
    "    \"\"\"\n",
    "    Converts LabelMe JSON annotations to binary masks.\n",
    "\n",
    "    Parameters:\n",
    "        labelme_dir (str): Directory containing LabelMe JSON files.\n",
    "        output_mask_dir (str): Directory to save generated masks.\n",
    "    \"\"\"\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_mask_dir, exist_ok=True)\n",
    "\n",
    "    # Clear any existing .jpg masks in the output directory\n",
    "    for mask_file in Path(output_mask_dir).glob(\"*.jpg\"):\n",
    "        os.remove(mask_file)\n",
    "\n",
    "    for json_file in Path(labelme_dir).glob(\"*.json\"):\n",
    "        with open(json_file, \"r\") as f:\n",
    "            label_data = json.load(f)\n",
    "\n",
    "        # Get image dimensions\n",
    "        img_height = label_data[\"imageHeight\"]\n",
    "        img_width = label_data[\"imageWidth\"]\n",
    "\n",
    "        # Create an empty mask\n",
    "        mask = np.zeros((img_height, img_width), dtype=np.uint8)\n",
    "\n",
    "        # Add polygons to the mask\n",
    "        for shape in label_data[\"shapes\"]:\n",
    "            if shape[\"label\"] == \"window\":  # Use only \"window\" labels\n",
    "                points = np.array(shape[\"points\"], dtype=np.int32)\n",
    "                cv2.fillPoly(mask, [points], color=1)\n",
    "\n",
    "        # Save the mask as .png\n",
    "        mask_name = Path(label_data[\"imagePath\"]).stem + \".png\"\n",
    "        mask_path = os.path.join(output_mask_dir, mask_name)\n",
    "        cv2.imwrite(mask_path, mask * 255)  # Convert to 0-255 scale for saving as PNG\n",
    "\n",
    "    print(f\"Masks saved in {output_mask_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to delete A_Hyu_01.jpg\n",
    "create_masks(\"../data/processed/scaled/labels\", \"../data/processed/scaled/masks\")\n",
    "output_scaled_mask_dir = \"../data/processed/scaled/masks\" \n",
    "augment_dataset(output_dir_images_scaled, output_scaled_mask_dir, output_augmentations_dir, num_augmented=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bounding_box(image_path, bounding_boxes):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    img_height, img_width = image.shape[:2]\n",
    "\n",
    "    # Define the colors\n",
    "    colors = {\n",
    "        0: (255, 0, 0),\n",
    "        1: (0, 255, 0),\n",
    "        2: (0, 0, 255),\n",
    "        3: (255, 255, 0),\n",
    "        4: (255, 0, 255),\n",
    "        5: (0, 255, 255),\n",
    "    }\n",
    "\n",
    "    for bbox in bounding_boxes:\n",
    "        class_id, x_center, y_center, width, height = bbox\n",
    "\n",
    "        # Convert the coordinates to absolute values\n",
    "        x_center, y_center = int(x_center * img_width), int(y_center * img_height)\n",
    "        width, height = int(width * img_width), int(height * img_height)\n",
    "\n",
    "        # Calculate the top-left and bottom-right corner of the bounding box\n",
    "        x1 = int(x_center - width / 2)\n",
    "        y1 = int(y_center - height / 2)\n",
    "        x2 = int(x_center + width / 2)\n",
    "        y2 = int(y_center + height / 2)\n",
    "\n",
    "        # Class color\n",
    "        color = colors[class_id]\n",
    "\n",
    "        # Draw the bounding box\n",
    "        image = cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)\n",
    "        # Add the class name\n",
    "        cv2.putText(\n",
    "            image, str(class_id), (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2\n",
    "        )\n",
    "\n",
    "    # Display the image\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_mask(image_path, mask_path):\n",
    "    \"\"\"\n",
    "    Visualizes the original image and its corresponding mask.\n",
    "\n",
    "    Parameters:\n",
    "        image_path (str): Path to the input image.\n",
    "        mask_path (str): Path to the binary mask.\n",
    "    \"\"\"\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image)\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(mask, cmap=\"gray\")\n",
    "    plt.title(\"Segmentation Mask\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = '../data/model_training/images/A_Ast_01.jpg'\n",
    "image_label_path = '../data/model_training/labels_txt/A_Ast_01.txt'\n",
    "\n",
    "bounding_boxes = []\n",
    "with open(image_label_path, 'r') as file:\n",
    "    for line in file:\n",
    "        class_id, x_center, y_center, width, height = line.strip().split()\n",
    "        bounding_boxes.append([int(class_id), float(x_center), float(y_center), float(width), float(height)])\n",
    "\n",
    "draw_bounding_box(image_path, bounding_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_labels_dir = \"../data/model_training/json_labels\"  # Directory containing LabelMe JSON files\n",
    "output_mask_dir = \"../data/model_training/masks\"  # Directory of masks\n",
    "\n",
    "# Create masks from JSON labels\n",
    "create_masks(json_labels_dir, output_mask_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the paths to visualize a different mask\n",
    "visualize_mask(\"../data/model_training/images/A_Ast_01.jpg\", \"../data/model_training/masks/A_Ast_01.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = '../data/processed/cropped/images/A_Ast_01.jpg'\n",
    "image_label_path = '../data/processed/cropped/labels/A_Ast_01.txt'\n",
    "\n",
    "bounding_boxes = []\n",
    "with open(image_label_path, 'r') as file:\n",
    "    for line in file:\n",
    "        class_id, x_center, y_center, width, height = line.strip().split()\n",
    "        bounding_boxes.append([int(class_id), float(x_center), float(y_center), float(width), float(height)])\n",
    "\n",
    "draw_bounding_box(image_path, bounding_boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Flipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_flipped_labels_dir = \"../data/processed/flipped/json_labels\" \n",
    "output_scaled_mask_dir = \"../data/processed/flipped/masks\"  \n",
    "\n",
    "# Create masks from JSON labels\n",
    "create_masks(json_flipped_labels_dir, output_scaled_mask_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the paths to visualize a different mask\n",
    "visualize_mask(\"../data/processed/flipped/images/A_Ast_01.jpg\", \"../data/processed/flipped/masks/A_Ast_01.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_augmented_data(image_dir, mask_dir, num_samples=5):\n",
    "    \"\"\"\n",
    "    Visualizes a few augmented image-mask pairs.\n",
    "\n",
    "    Parameters:\n",
    "        image_dir (str): Directory containing augmented images.\n",
    "        mask_dir (str): Directory containing augmented masks.\n",
    "        num_samples (int): Number of samples to visualize.\n",
    "    \"\"\"\n",
    "    image_paths = sorted(Path(image_dir).glob(\"*.jpg\"))[:num_samples]\n",
    "    mask_paths = sorted(Path(mask_dir).glob(\"*.png\"))[:num_samples]\n",
    "\n",
    "    for img_path, mask_path in zip(image_paths, mask_paths):\n",
    "        image = cv2.imread(str(img_path))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(image)\n",
    "        plt.title(f\"Augmented Image: {img_path.name}\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(mask, cmap=\"gray\")\n",
    "        plt.title(f\"Augmented Mask: {mask_path.name}\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Augmentation visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_images_dir = \"../data/processed/augmentations/images\"\n",
    "augmented_masks_dir = \"../data/processed/augmentations/masks\"\n",
    "visualize_augmented_data(augmented_images_dir, augmented_masks_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Splitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "augmented_images_dir = \"../data/processed/augmentations/images\"\n",
    "augmented_masks_dir = \"../data/processed/augmentations/masks\"\n",
    "output_split_dir = \"../data/model_training/split\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(images_dir, masks_dir, output_dir, train_ratio=0.7, val_ratio=0.2):\n",
    "    \"\"\"\n",
    "    Splits the augmented dataset into train, val, and test sets.\n",
    "\n",
    "    Parameters:\n",
    "        images_dir (str): Directory containing augmented images.\n",
    "        masks_dir (str): Directory containing augmented masks.\n",
    "        output_dir (str): Base directory for the split datasets.\n",
    "        train_ratio (float): Proportion of data for training (default: 0.7).\n",
    "        val_ratio (float): Proportion of data for validation (default: 0.2).\n",
    "    \"\"\"\n",
    "    # Get all images and masks\n",
    "    images = sorted(Path(images_dir).glob(\"*.jpg\"))\n",
    "    masks = sorted(Path(masks_dir).glob(\"*.png\"))\n",
    "\n",
    "    # Ensure images and masks match\n",
    "    assert len(images) == len(masks), \"Number of images and masks do not match!\"\n",
    "    for img, mask in zip(images, masks):\n",
    "        assert img.stem == mask.stem, f\"Mismatch: {img.stem} and {mask.stem}\"\n",
    "\n",
    "    # Split dataset into train, val, and test\n",
    "    train_images, temp_images, train_masks, temp_masks = train_test_split(\n",
    "        images, masks, test_size=(1 - train_ratio), random_state=42\n",
    "    )\n",
    "    val_ratio_adjusted = val_ratio / (1 - train_ratio)  # Adjust for remaining data\n",
    "    val_images, test_images, val_masks, test_masks = train_test_split(\n",
    "        temp_images, temp_masks, test_size=(1 - val_ratio_adjusted), random_state=42\n",
    "    )\n",
    "\n",
    "    # Define output directories\n",
    "    splits = {\"train\": (train_images, train_masks), \n",
    "              \"val\": (val_images, val_masks), \n",
    "              \"test\": (test_images, test_masks)}\n",
    "\n",
    "    for split, (split_images, split_masks) in splits.items():\n",
    "        split_image_dir = Path(output_dir) / split / \"images\"\n",
    "        split_mask_dir = Path(output_dir) / split / \"masks\"\n",
    "        split_image_dir.mkdir(parents=True, exist_ok=True)\n",
    "        split_mask_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Move files to respective directories\n",
    "        for img, mask in zip(split_images, split_masks):\n",
    "            shutil.copy(img, split_image_dir / img.name)\n",
    "            shutil.copy(mask, split_mask_dir / mask.name)\n",
    "\n",
    "    print(f\"Dataset split into train, val, and test sets in {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Split execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset(augmented_images_dir, augmented_masks_dir, output_split_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_split(split_dir, num_samples=1):\n",
    "    images_dir = Path(split_dir) / \"images\"\n",
    "    masks_dir = Path(split_dir) / \"masks\"\n",
    "\n",
    "    image_paths = sorted(images_dir.glob(\"*.jpg\"))[:num_samples]\n",
    "    mask_paths = sorted(masks_dir.glob(\"*.png\"))[:num_samples]\n",
    "\n",
    "    for img_path, mask_path in zip(image_paths, mask_paths):\n",
    "        image = cv2.imread(str(img_path))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(image)\n",
    "        plt.title(f\"Image: {img_path.name}\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(mask, cmap=\"gray\")\n",
    "        plt.title(f\"Mask: {mask_path.name}\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_images(images_path):\n",
    "    return len(\n",
    "        [\n",
    "            f\n",
    "            for f in os.listdir(images_path)\n",
    "            if f.endswith(\".jpg\") and not f.startswith(\".\")\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Split visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_split(\"../data/model_training/split/train\")\n",
    "visualize_split(\"../data/model_training/split/val\")\n",
    "visualize_split(\"../data/model_training/split/test\")\n",
    "\n",
    "train_images = get_num_images(images_path=\"../data/model_training/split/train/images\")\n",
    "val_images = get_num_images(images_path=\"../data/model_training/split/val/images\")\n",
    "test_images = get_num_images(images_path=\"../data/model_training/split/test/images\")\n",
    "\n",
    "print(f\"Number of training images: {train_images}\")\n",
    "print(f\"Number of validation images: {val_images}\")\n",
    "print(f\"Number of testing images: {test_images}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
