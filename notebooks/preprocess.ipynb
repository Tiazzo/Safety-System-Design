{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess\n",
    "This notebook is responsible for preprocessing the images.\n",
    "\n",
    "Contents:\n",
    "- Crop (Remove background noise)\n",
    "- Flip (Every car should be in the same direction)\n",
    "- Scale (Scale all cars according to a reference car)\n",
    "- Execute (Run the scripts)\n",
    "\n",
    "What will you need to do?\n",
    "- Create a dir named *data* in the root folder of the project\n",
    "- Download the GP22 Dataset (https://zenodo.org/records/6366808), both Images.zip and Labels.zip\n",
    "- Run code block below to generate directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure paths here \n",
    "path_to_orientation_model = \"../data/models/orientation_model/best_model.pth\"\n",
    "path_to_gp22_images = \"../data/GP22/images\"\n",
    "path_to_gp22_labels = \"../data/GP22/labels\"\n",
    "path_to_json_labels = \"../data/json_labels\"\n",
    "\n",
    "# path_to_reference_car_label # Configured under 3rd code block in --> Execute\n",
    "output_dir_images_cropped = \"../data/processed/cropped\"\n",
    "output_dir_images_flipped = \"../data/processed/flipped_images\"\n",
    "output_dir_labels_flipped = \"../data/processed/flipped_labels\"\n",
    "output_dir_images_scaled = \"../data/processed/scaled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import zipfile\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import torch\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate directories if they do not already exist\n",
    "def ensure_directories_exist(paths):\n",
    "    for path in paths:\n",
    "        # If the path is a file, get its directory. Otherwise, use the path as-is.\n",
    "        directory = path if path.endswith('/') or '.' not in os.path.basename(path) else os.path.dirname(path)\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "            print(f\"Created directory: {directory}\")\n",
    "        else:\n",
    "            print(f\"Directory already exists: {directory}\")\n",
    "\n",
    "# Configure paths here\n",
    "paths = [\n",
    "    \"../data\",\n",
    "    \"../data/models/orientation_model/best_model.pth\",\n",
    "    \"../data/GP22/images/\",\n",
    "    \"../data/GP22/labels/\",\n",
    "    \"../data/json_labels/\",\n",
    "    \"../data/processed/cropped/\",\n",
    "    \"../data/processed/flipped_images/\",\n",
    "    \"../data/processed/flipped_labels/\",\n",
    "    \"../data/processed/scaled\"\n",
    "]\n",
    "\n",
    "ensure_directories_exist(paths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unzip of GP22 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert the downloaded images.zip and labels.zip into the data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "base_dir = \"../data/GP22\"\n",
    "images_zip = \"../data/Images.zip\"\n",
    "labels_zip = \"../data/Labels.zip\"\n",
    "images_dir = os.path.join(base_dir, \"images\")\n",
    "labels_dir = os.path.join(base_dir, \"labels\")\n",
    "\n",
    "# Function to remove __MACOSX folder if it exists\n",
    "def remove_macosx_folder(base_dir):\n",
    "    macosx_path = os.path.join(base_dir, \"__MACOSX\")\n",
    "    if os.path.exists(macosx_path):\n",
    "        shutil.rmtree(macosx_path)\n",
    "        print(f\"Removed {macosx_path}.\")\n",
    "\n",
    "# Check and unzip Images.zip\n",
    "if os.path.exists(images_zip):\n",
    "    if not os.listdir(images_dir):\n",
    "        print(f\"Extracting {images_zip} to {images_dir}...\")\n",
    "        with zipfile.ZipFile(images_zip, 'r') as zip_ref:\n",
    "            zip_ref.extractall(base_dir)  # Extract to base_dir, assuming 'images' is already created\n",
    "        print(f\"Extraction complete: {images_dir}\")\n",
    "        os.remove(images_zip)\n",
    "        print(f\"Deleted {images_zip}.\")\n",
    "        remove_macosx_folder(base_dir)  # Remove __MACOSX if it exists\n",
    "    else:\n",
    "        print(f\"{images_dir} already contains files. Skipping extraction of {images_zip}.\")\n",
    "else:\n",
    "    print(f\"{images_zip} not found.\")\n",
    "\n",
    "# Check and unzip Labels.zip\n",
    "if os.path.exists(labels_zip):\n",
    "    if not os.listdir(labels_dir):\n",
    "        print(f\"Extracting {labels_zip} to {labels_dir}...\")\n",
    "        with zipfile.ZipFile(labels_zip, 'r') as zip_ref:\n",
    "            zip_ref.extractall(base_dir)  # Extract to base_dir, assuming 'labels' is already created\n",
    "        print(f\"Extraction complete: {labels_dir}\")\n",
    "        os.remove(labels_zip)\n",
    "        print(f\"Deleted {labels_zip}.\")\n",
    "        remove_macosx_folder(base_dir)  # Remove __MACOSX if it exists\n",
    "    else:\n",
    "        print(f\"{labels_dir} already contains files. Skipping extraction of {labels_zip}.\")\n",
    "else:\n",
    "    print(f\"{labels_zip} not found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crop\n",
    "Removing background noise slightly improves model performance.\n",
    "The following code block will: \n",
    "- Use GP22 labels to process the flipped images and remove background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_out_background(images_dir, labels_dir, output_dir):\n",
    "    \"\"\"\n",
    "    Process all images in a directory and crop out the background using bounding boxes.\n",
    "\n",
    "    Parameters:\n",
    "        images_dir (str): Directory containing the images.\n",
    "        labels_dir (str): Directory containing the bounding box labels in text format.\n",
    "        output_dir (str): Directory to save the cropped images.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Loop through all image files in the images_dir\n",
    "    for image_path in Path(images_dir).glob(\"*.jpg\"):\n",
    "        # Get the corresponding label file\n",
    "        label_path = Path(labels_dir) / f\"{image_path.stem}.txt\"\n",
    "\n",
    "        # Check if the label file exists\n",
    "        if not label_path.exists():\n",
    "            print(f\"No label found for {image_path.name}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Load the image\n",
    "        image = cv2.imread(str(image_path))\n",
    "        if image is None:\n",
    "            print(f\"Could not read the image at {image_path}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Get image dimensions\n",
    "        img_height, img_width = image.shape[:2]\n",
    "\n",
    "        # Create a blank mask with the same dimensions as the image\n",
    "        mask = np.zeros(image.shape[:2], dtype=np.uint8)\n",
    "\n",
    "        # Read the bounding boxes from the label file\n",
    "        bounding_boxes = []\n",
    "        with open(label_path, 'r') as file:\n",
    "            for line in file:\n",
    "                class_id, x_center, y_center, width, height = map(float, line.strip().split())\n",
    "                bounding_boxes.append([x_center, y_center, width, height])\n",
    "\n",
    "        # Draw white rectangles for each bounding box on the mask\n",
    "        for (x_center, y_center, width, height) in bounding_boxes:\n",
    "            # Convert normalized coordinates to pixel values\n",
    "            x = int((x_center - width / 2) * img_width)\n",
    "            y = int((y_center - height / 2) * img_height)\n",
    "            w = int(width * img_width)\n",
    "            h = int(height * img_height)\n",
    "\n",
    "            # Draw the rectangle on the mask\n",
    "            cv2.rectangle(mask, (x, y), (x + w, y + h), 255, thickness=-1)\n",
    "\n",
    "        # Apply the mask to the image\n",
    "        result = cv2.bitwise_and(image, image, mask=mask)\n",
    "\n",
    "        # Save the result to the output directory\n",
    "        output_path = Path(output_dir) / image_path.name\n",
    "        cv2.imwrite(str(output_path), result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flip\n",
    "Every car should be pointing in the same direction.\n",
    "The following code blocks will:\n",
    "- Load the orientation model\n",
    "- Predict whether a car is pointing left or right\n",
    "- Flip the corresponding label of each car\n",
    "- Flip the image of the car itself\n",
    "\n",
    "We have chosen to flip cars to the **left**. That is, nose points to the left."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the orientation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "# Set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_orientation_model(checkpoint_path):\n",
    "    \"\"\"\n",
    "    Loads the trained orientation model from the given checkpoint path.\n",
    "    \"\"\"\n",
    "    model = models.resnet18(pretrained=False)\n",
    "    model.fc = torch.nn.Linear(model.fc.in_features, 2)  # Binary classification: left (0) vs. right (1)\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    print(f\"Model loaded from {checkpoint_path}\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict orientation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_orientation(model, image_path):\n",
    "    \"\"\"\n",
    "    Predicts the orientation of the car in the image.\n",
    "    Returns 'left' if the car faces left, 'right' if it faces right.\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(image_tensor)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        return \"left\" if predicted.item() == 0 else \"right\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flip x-coordinates of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_labels_x(label_path, output_label_dir):\n",
    "    \"\"\"\n",
    "    Flips the x-coordinates of labels and saves them to a new directory.\n",
    "\n",
    "    Parameters:\n",
    "        label_path (str): Path to the original label file.\n",
    "        output_label_dir (str): Directory to save the flipped labels.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_label_dir, exist_ok=True)\n",
    "\n",
    "    flipped_labels = []\n",
    "    with open(label_path, 'r') as f:\n",
    "        for line in f:\n",
    "            class_id, x_center, y_center, width, height = map(float, line.strip().split())\n",
    "            x_center = 1 - x_center  # Flip the x-coordinate\n",
    "            flipped_labels.append(f\"{int(class_id)} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\")\n",
    "\n",
    "    output_label_path = os.path.join(output_label_dir, os.path.basename(label_path))\n",
    "    with open(output_label_path, 'w') as f:\n",
    "        f.write(\"\\n\".join(flipped_labels))\n",
    "    \n",
    "    print(f\"Flipped labels saved to {output_label_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flip images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_image(image_path, output_image_dir):\n",
    "    \"\"\"\n",
    "    Flips an image horizontally and saves it to the output directory.\n",
    "\n",
    "    Parameters:\n",
    "        image_path (str): Path to the original image.\n",
    "        output_image_dir (str): Directory to save the flipped image.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_image_dir, exist_ok=True)\n",
    "\n",
    "    image = cv2.imread(str(image_path))\n",
    "    flipped_image = cv2.flip(image, 1)\n",
    "    output_image_path = os.path.join(output_image_dir, os.path.basename(image_path))\n",
    "    cv2.imwrite(output_image_path, flipped_image)\n",
    "    \n",
    "    print(f\"Flipped image saved to {output_image_path}\")\n",
    "    return output_image_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flip Images and Labels\n",
    "Run this to process all images and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_images_and_labels(model, images_dir, labels_dir, output_image_dir, output_label_dir):\n",
    "    \"\"\"\n",
    "    Processes all images: identifies flipped images, flips them, and flips the labels' x-coordinates.\n",
    "\n",
    "    Parameters:\n",
    "        model (torch.nn.Module): Trained orientation model.\n",
    "        images_dir (str): Directory containing the input images.\n",
    "        labels_dir (str): Directory containing the label files.\n",
    "        output_image_dir (str): Directory to save processed images.\n",
    "        output_label_dir (str): Directory to save flipped labels.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_image_dir, exist_ok=True)\n",
    "    os.makedirs(output_label_dir, exist_ok=True)\n",
    "\n",
    "    for image_path in Path(images_dir).glob(\"*.jpg\"):\n",
    "        label_path = Path(labels_dir) / f\"{image_path.stem}.txt\"\n",
    "        \n",
    "        if label_path.exists():\n",
    "            # Predict orientation\n",
    "            orientation = predict_orientation(model, str(image_path))\n",
    "            flipped = False\n",
    "\n",
    "            if orientation == \"right\":\n",
    "                flipped = True\n",
    "                # Flip the image and save\n",
    "                flip_image(str(image_path), output_image_dir)\n",
    "                # Flip the labels and save\n",
    "                flip_labels_x(str(label_path), output_label_dir)\n",
    "            else:\n",
    "                # Copy the original image and label to the output directories\n",
    "                shutil.copy(str(image_path), os.path.join(output_image_dir, image_path.name))\n",
    "                shutil.copy(str(label_path), os.path.join(output_label_dir, label_path.name))\n",
    "                print(f\"Image and labels copied to {output_image_dir} and {output_label_dir}\")\n",
    "        else:\n",
    "            print(f\"Label file not found for {image_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale\n",
    "Cars should be relative to each other in size.\n",
    "The following code blocks will:\n",
    "- Calculate rim area (used as a reference when scaling)\n",
    "- Calculate scaling factor (based on reference car and current car)\n",
    "- Scale images and update labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rim area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rim_area_of_front_wheel(label_path, resolution=1024):\n",
    "    \"\"\"\n",
    "    Returns {area} of front-wheel rim\n",
    "    Important! Car nose should be pointing left\n",
    "\n",
    "    Params:\n",
    "    label_path: path of .txt files in Yolo format where class_id = 1 => rim\n",
    "    resolution: resolution of image, default = 1024 (GP22 Dataset Standard)\n",
    "    \"\"\"\n",
    "    smallest_x = float(\"inf\")\n",
    "    smallest_box = None\n",
    "\n",
    "    with open(label_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            class_id, x_center, y_center, width, height = line.strip().split()\n",
    "            if int(class_id) == 1:  # Only consider class_id = 1 (rim)\n",
    "                x_center = float(x_center)\n",
    "                if x_center < smallest_x:\n",
    "                    smallest_x = x_center\n",
    "                    smallest_box = (float(width), float(height))\n",
    "\n",
    "    if smallest_box:\n",
    "        width, height = smallest_box\n",
    "        area = width * height * (resolution**2)\n",
    "        return area\n",
    "    else:\n",
    "        return 0  # Return 0 if no class_id = 1 boxes are found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_area(rim_area_reference_car, rim_area_current_car):\n",
    "    \"\"\"\n",
    "    Calculate the scaling factor to adjust the dimensions of the current car\n",
    "    so that its area matches the area of the reference car.\n",
    "\n",
    "    Parameters:\n",
    "    - rim_area_reference_car (float): The area of the reference car's bounding box.\n",
    "    - rim_area_current_car (float): The area of the current car's bounding box.\n",
    "\n",
    "    Returns:\n",
    "    - scaling_factor (float): The factor by which the current car's dimensions \n",
    "                              (width and height) should be scaled.\n",
    "    \"\"\"\n",
    "    if rim_area_reference_car <= 0 or rim_area_current_car <= 0:\n",
    "        raise ValueError(\"Both areas must be positive numbers.\")\n",
    "    \n",
    "    scaling_factor = (rim_area_reference_car / rim_area_current_car) ** 0.5\n",
    "    \n",
    "    return scaling_factor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale JSON labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_json_labels(json_path, scaling_factor, offset_x, offset_y, labels_dir):\n",
    "    \"\"\"\n",
    "    Updates the JSON labels to match the scaled and padded image.\n",
    "\n",
    "    Parameters:\n",
    "    - json_path (str): Path to the JSON file containing label data.\n",
    "    - scaling_factor (float): The factor by which the image dimensions were scaled.\n",
    "    - offset_x (int): The x-offset applied during padding.\n",
    "    - offset_y (int): The y-offset applied during padding.\n",
    "    - labels_dir (str): Directory to save the updated JSON.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    with open(json_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Scale and offset points\n",
    "    for shape in data['shapes']:\n",
    "        new_points = []\n",
    "        for point in shape['points']:\n",
    "            # First scale the points\n",
    "            scaled_x = point[0] * scaling_factor\n",
    "            scaled_y = point[1] * scaling_factor\n",
    "            \n",
    "            # Then add the positive offset to center the points\n",
    "            scaled_x += offset_x  # Remove the negative sign\n",
    "            scaled_y += offset_y  # Remove the negative sign\n",
    "\n",
    "            # Only include points that fall within the 1024x1024 canvas\n",
    "            if 0 <= scaled_x < 1024 and 0 <= scaled_y < 1024:\n",
    "                new_points.append([scaled_x, scaled_y])\n",
    "\n",
    "        shape['points'] = new_points\n",
    "\n",
    "    # Update image metadata\n",
    "    data['imageWidth'] = 1024\n",
    "    data['imageHeight'] = 1024\n",
    "\n",
    "    # Save updated JSON\n",
    "    json_filename = os.path.basename(json_path)\n",
    "    updated_json_path = os.path.join(labels_dir, json_filename)\n",
    "\n",
    "    with open(updated_json_path, 'w') as file:\n",
    "        json.dump(data, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_and_crop_image(image_path, scaling_factor, label_path, json_label_path, output_dir):\n",
    "    \"\"\"\n",
    "    Scales an image by a given scaling factor, ensures it is padded to 1024x1024 pixels,\n",
    "    and scales corresponding JSON labels.\n",
    "    \"\"\"\n",
    "    if scaling_factor <= 0:\n",
    "        raise ValueError(\"Scaling factor must be a positive number.\")\n",
    "\n",
    "    # Read the image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        raise FileNotFoundError(f\"Image file not found: {image_path}\")\n",
    "\n",
    "    # Get original dimensions\n",
    "    original_height, original_width = image.shape[:2]\n",
    "\n",
    "    # Compute new dimensions\n",
    "    new_width = int(original_width * scaling_factor)\n",
    "    new_height = int(original_height * scaling_factor)\n",
    "\n",
    "    # Resize the image using OpenCV\n",
    "    scaled_image = cv2.resize(\n",
    "        image, (new_width, new_height), interpolation=cv2.INTER_LINEAR\n",
    "    )\n",
    "\n",
    "    # Ensure the image is 1024x1024 by padding with black if necessary\n",
    "    target_size = 1024\n",
    "    padded_image = np.zeros((target_size, target_size, 3), dtype=np.uint8)\n",
    "\n",
    "    # Center the scaled image in the 1024x1024 canvas\n",
    "    offset_x = (target_size - new_width) // 2 if new_width < target_size else 0\n",
    "    offset_y = (target_size - new_height) // 2 if new_height < target_size else 0\n",
    "\n",
    "    insert_width = min(new_width, target_size)\n",
    "    insert_height = min(new_height, target_size)\n",
    "\n",
    "    padded_image[offset_y:offset_y+insert_height, offset_x:offset_x+insert_width] = \\\n",
    "        scaled_image[:insert_height, :insert_width]\n",
    "\n",
    "    # Create directories for images and labels\n",
    "    images_dir = os.path.join(output_dir, \"images\")\n",
    "    labels_dir = os.path.join(output_dir, \"labels\")\n",
    "    os.makedirs(images_dir, exist_ok=True)\n",
    "    os.makedirs(labels_dir, exist_ok=True)\n",
    "\n",
    "    # Save padded image\n",
    "    image_filename = os.path.basename(image_path)\n",
    "    padded_image_path = os.path.join(images_dir, image_filename)\n",
    "    cv2.imwrite(padded_image_path, padded_image)\n",
    "\n",
    "    # Update JSON labels\n",
    "    update_json_labels(json_label_path, scaling_factor, offset_x, offset_y, labels_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute\n",
    "The following code blocks will execute corresponding code for:\n",
    "- Removing background\n",
    "- Flipping images and labels\n",
    "- Scaling images and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crop\n",
    "path_to_gp22_images          # Directory with GP22 images\n",
    "path_to_gp22_labels          # Directory with bounding box labels (text files)\n",
    "output_dir_images_cropped    # Directory to save cropped images\n",
    "\n",
    "crop_out_background(path_to_gp22_images, path_to_gp22_labels, output_dir_images_cropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flip\n",
    "path_to_orientation_model   # Directory with orientation model\n",
    "output_dir_images_cropped   # Directory with cropped images\n",
    "path_to_json_labels         # Directory with JSON labels\n",
    "output_dir_images_flipped   # Directory to save flipped images\n",
    "output_dir_labels_flipped   # Directory to save flipped labels\n",
    "\n",
    "model = load_orientation_model(path_to_orientation_model)\n",
    "\n",
    "flip_images_and_labels(\n",
    "    model, \n",
    "    output_dir_images_cropped, \n",
    "    path_to_json_labels, \n",
    "    output_dir_images_flipped, \n",
    "    output_dir_labels_flipped\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale\n",
    "def scale_all_images_and_labels(reference_label_path, images_folder, labels_folder, json_labels_folder, output_folder, resolution=1024):\n",
    "    \"\"\"\n",
    "    Scales images and corresponding labels\n",
    "    \"\"\"\n",
    "    # Create output directories\n",
    "    images_output_dir = os.path.join(output_folder, \"images\")\n",
    "    labels_output_dir = os.path.join(output_folder, \"labels\")\n",
    "    os.makedirs(images_output_dir, exist_ok=True)\n",
    "    os.makedirs(labels_output_dir, exist_ok=True)\n",
    "\n",
    "    # Ensure the reference label path is valid\n",
    "    if not os.path.exists(reference_label_path):\n",
    "        print(f\"Reference label path {reference_label_path} does not exist.\")\n",
    "        return\n",
    "    \n",
    "    reference_area = calculate_rim_area_of_front_wheel(reference_label_path, resolution)\n",
    "    print(f\"Reference area: {reference_area}\")\n",
    "    \n",
    "    # Loop through label files in the label folder\n",
    "    print(f\"Looking for label files in: {labels_folder}\")\n",
    "    for label_file in os.listdir(labels_folder):\n",
    "        if label_file.endswith(\".txt\"):\n",
    "            print(f\"Found label file: {label_file}\")\n",
    "            label_path = os.path.join(labels_folder, label_file)\n",
    "            \n",
    "            current_area = calculate_rim_area_of_front_wheel(label_path, resolution)\n",
    "            print(f\"Current area for {label_file}: {current_area}\")\n",
    "            \n",
    "            # Get corresponding image file\n",
    "            image_name = label_file.replace(\".txt\", \"_aug_0.jpg\")  # Assuming images are .jpg\n",
    "            image_path = Path(images_folder) / image_name  # Use Path to handle path joining\n",
    "            image_path = str(image_path).replace(\"\\\\\", \"/\")\n",
    "            print(f\"Looking for image: {image_path}\")\n",
    "            \n",
    "            if not os.path.exists(image_path):\n",
    "                print(f\"Image {image_name} corresponding to label {label_file} does not exist. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Get corresponding JSON label file (assumes the JSON label has the same name as the .txt label)\n",
    "            json_label_name = label_file.replace(\".txt\", \".json\")\n",
    "            json_label_path = os.path.join(json_labels_folder, json_label_name)\n",
    "            print(f\"Looking for JSON label: {json_label_path}\")\n",
    "            \n",
    "            if not os.path.exists(json_label_path):\n",
    "                print(f\"JSON label {json_label_name} for {label_file} does not exist. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            print(f\"Processing: {image_name} and {label_file}\")\n",
    "\n",
    "            scaling_factor = compare_area(reference_area, current_area)\n",
    "            print(f\"Scaling factor for {label_file}: {scaling_factor}\")\n",
    "\n",
    "            # Process the image and label\n",
    "            try:\n",
    "                scale_and_crop_image(image_path, scaling_factor, label_path, json_label_path, output_folder)\n",
    "                print(f\"Processed and saved: {image_name} and {label_file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {image_name} and {label_file}: {e}\")\n",
    "\n",
    "\n",
    "path_to_reference_car_label = \"../data/GP22/labels/B_Ren_12.txt\" # Directory to chosen reference car\n",
    "output_dir_images_flipped   # Directory to images of flipped cars\n",
    "output_dir_labels_flipped   # Directory to labels of flipped cars\n",
    "output_dir_images_scaled           # Directory to save images and labels of scaled cars\n",
    "\n",
    "scale_all_images_and_labels(\n",
    "    path_to_reference_car_label, \n",
    "    output_dir_images_flipped, \n",
    "    path_to_gp22_labels, \n",
    "    output_dir_labels_flipped, \n",
    "    output_dir_images_scaled\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
