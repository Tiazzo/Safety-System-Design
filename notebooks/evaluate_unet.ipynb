{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation with U-NET \n",
    "Run this notebook to see perform the evaluation of the U-Net model\n",
    "\n",
    "## Initialization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import utils\n",
    "import cv2\n",
    "import os \n",
    "import sys\n",
    "import matplotlib.pyplot as plt \n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from albumentations import Resize, Compose, Normalize, HorizontalFlip, RandomBrightnessContrast\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "\n",
    "# import functions\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from src.utils import *\n",
    "from src.dataset_manager import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, images_dir, masks_dir, transform=None):\n",
    "        self.images_dir = images_dir\n",
    "        self.masks_dir = masks_dir\n",
    "        self.image_paths = sorted(list(Path(images_dir).glob(\"*.jpg\")))\n",
    "        self.mask_paths = sorted(list(Path(masks_dir).glob(\"*.png\")))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        mask_path = self.mask_paths[idx]\n",
    "\n",
    "        # Load image and mask\n",
    "        image = cv2.imread(str(image_path))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented[\"image\"]\n",
    "            mask = augmented[\"mask\"]\n",
    "\n",
    "        # Normalize mask (binary: 0 or 1)\n",
    "        mask = torch.tensor(mask, dtype=torch.float32).unsqueeze(0) / 255.0\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training augmentations\n",
    "train_transform = Compose(\n",
    "    [\n",
    "        HorizontalFlip(p=0.5),\n",
    "        RandomBrightnessContrast(p=0.2),\n",
    "        Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        Resize(512, 512),  # Resize to 512x512 (adjust based on model input size)\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Validation augmentations\n",
    "val_transform = Compose(\n",
    "    [\n",
    "        Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        Resize(512, 512),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_dir = \"../data/model_training/split/train/images\"\n",
    "train_masks_dir = \"../data/model_training/split/train/masks\"\n",
    "val_images_dir = \"../data/model_training/split/val/images\"\n",
    "val_masks_dir = \"../data/model_training/split/val/masks\"\n",
    "\n",
    "train_dataset = SegmentationDataset(train_images_dir, train_masks_dir, transform=train_transform)\n",
    "val_dataset = SegmentationDataset(val_images_dir, val_masks_dir, transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the U-Net model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from segmentation_models_pytorch import Unet\n",
    "\n",
    "model = Unet(\n",
    "    encoder_name=\"resnet34\",  # Encoder backbone\n",
    "    encoder_weights=None,    # No pretraining needed when loading weights\n",
    "    in_channels=3,           # Input channels (RGB)\n",
    "    classes=1,               # Output channels (binary segmentation)\n",
    ")\n",
    "\n",
    "# Load the model weights\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_path = \"../checkpoints/test_best_unet_model.pth\"  # Path to the saved model\n",
    "state_dict = torch.load(model_path, map_location=device)  \n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute evaluation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from scipy.spatial.distance import directed_hausdorff\n",
    "\n",
    "def compute_metrics(model, dataloader):\n",
    "    model.eval()\n",
    "    iou_scores = []\n",
    "    dice_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "    hausdorff_distances = []\n",
    "    mae_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks in dataloader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            outputs = model(images)\n",
    "            preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "\n",
    "            # Compute metrics for each sample\n",
    "            for pred, mask in zip(preds, masks):\n",
    "                intersection = (pred * mask).sum().item()\n",
    "                pred_sum = pred.sum().item()\n",
    "                mask_sum = mask.sum().item()\n",
    "                union = pred_sum + mask_sum - intersection\n",
    "\n",
    "                # IoU\n",
    "                iou = intersection / (union + 1e-6)\n",
    "                iou_scores.append(iou)\n",
    "\n",
    "                # Dice\n",
    "                dice = 2 * intersection / (pred_sum + mask_sum + 1e-6)\n",
    "                dice_scores.append(dice)\n",
    "\n",
    "                # Precision and Recall\n",
    "                tp = intersection\n",
    "                fp = pred_sum - intersection\n",
    "                fn = mask_sum - intersection\n",
    "                precision = tp / (tp + fp + 1e-6)\n",
    "                recall = tp / (tp + fn + 1e-6)\n",
    "                precision_scores.append(precision)\n",
    "                recall_scores.append(recall)\n",
    "\n",
    "                # F1 Score\n",
    "                f1 = 2 * precision * recall / (precision + recall + 1e-6)\n",
    "                f1_scores.append(f1)\n",
    "\n",
    "                 # Mean Absolute Error (MAE)\n",
    "                mae = torch.abs(pred - mask).mean().item()\n",
    "                mae_scores.append(mae)\n",
    "\n",
    "                # Hausdorff Distance\n",
    "                pred_indices = torch.nonzero(pred, as_tuple=False).cpu().numpy()\n",
    "                mask_indices = torch.nonzero(mask, as_tuple=False).cpu().numpy()\n",
    "                if len(pred_indices) > 0 and len(mask_indices) > 0:\n",
    "                    hd = max(\n",
    "                        directed_hausdorff(pred_indices, mask_indices)[0],\n",
    "                        directed_hausdorff(mask_indices, pred_indices)[0],\n",
    "                    )\n",
    "                else:\n",
    "                    hd = float(\"inf\")  # Handle case with empty prediction or mask\n",
    "                hausdorff_distances.append(hd)\n",
    "\n",
    "    \n",
    "\n",
    "    # Calculate mean metrics\n",
    "    mean_iou = np.mean(iou_scores)\n",
    "    mean_dice = np.mean(dice_scores)\n",
    "    mean_precision = np.mean(precision_scores)\n",
    "    mean_recall = np.mean(recall_scores)\n",
    "    mean_f1 = np.mean(f1_scores)\n",
    "    mean_mae = np.mean(mae_scores)\n",
    "    mean_hausdorff = np.mean([d for d in hausdorff_distances if d != float(\"inf\")])\n",
    "\n",
    "\n",
    "    print(f\"Mean IoU: {mean_iou:.4f}\")\n",
    "    print(f\"Mean Dice: {mean_dice:.4f}\")\n",
    "    print(f\"Mean Precision: {mean_precision:.4f}\")\n",
    "    print(f\"Mean Recall: {mean_recall:.4f}\")\n",
    "    print(f\"Mean F1-Score: {mean_f1:.4f}\")\n",
    "    print(f\"Mean MAE: {mean_mae:.4f}\")\n",
    "    print(f\"Mean Hausdorff Distance: {mean_hausdorff:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"iou\": mean_iou,\n",
    "        \"dice\": mean_dice,\n",
    "        \"precision\": mean_precision,\n",
    "        \"recall\": mean_recall,\n",
    "        \"f1\": mean_f1,\n",
    "        \"mae\": mean_mae,\n",
    "        \"hausdorff\": mean_hausdorff,\n",
    "    }\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_images_dir = \"../data/model_training/split/test/images\"\n",
    "test_masks_dir = \"../data/model_training/split/test/masks\"\n",
    "test_dataset = SegmentationDataset(\n",
    "    test_images_dir, test_masks_dir, transform=val_transform\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "metrics = compute_metrics(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the best and worse prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_and_worst_predictions_unet(model, dataloader):\n",
    "    \"\"\"\n",
    "    Evaluate U-Net model on the test dataset and find the best and worst predictions.\n",
    "    Args:\n",
    "        model: U-Net model.\n",
    "        dataloader: DataLoader for the test dataset.\n",
    "    Returns:\n",
    "        None (displays the best and worst predictions with their masks).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    best_iou = -float(\"inf\")\n",
    "    worst_iou = float(\"inf\")\n",
    "    best_data = None\n",
    "    worst_data = None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (images, ground_truth_mask) in enumerate(dataloader):\n",
    "            images, ground_truth_mask = images.to(device), ground_truth_mask.to(device)\n",
    "\n",
    "            # Predict using U-Net\n",
    "            outputs = model(images)\n",
    "            preds = (torch.sigmoid(outputs) > 0.5).float()  # Binarize predictions\n",
    "\n",
    "            # Compute IoU for each image in the batch\n",
    "            for i in range(images.size(0)):\n",
    "                pred = preds[i]\n",
    "                gt_mask = ground_truth_mask[i]\n",
    "\n",
    "                intersection = (pred * gt_mask).sum().item()\n",
    "                union = (pred + gt_mask).sum().item() - intersection\n",
    "                iou = intersection / (union + 1e-6)\n",
    "\n",
    "                # Update best and worst predictions\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_data = (images[i], pred, gt_mask, idx, iou)\n",
    "\n",
    "                if iou < worst_iou:\n",
    "                    worst_iou = iou\n",
    "                    worst_data = (images[i], pred, gt_mask, idx, iou)\n",
    "\n",
    "    # Display results\n",
    "    display_prediction(\"Best Prediction\", best_data)\n",
    "    display_prediction(\"Worst Prediction\", worst_data)\n",
    "\n",
    "\n",
    "def display_prediction(title, data):\n",
    "    \"\"\"\n",
    "    Display a single prediction with ground truth and mask.\n",
    "    Args:\n",
    "        title: Title of the display (e.g., 'Best Prediction').\n",
    "        data: Tuple containing image, prediction mask, ground truth mask, index, and IoU.\n",
    "    Returns:\n",
    "        None (displays the image with masks).\n",
    "    \"\"\"\n",
    "    if data is None:\n",
    "        print(f\"No {title} found.\")\n",
    "        return\n",
    "\n",
    "    image, prediction_mask, ground_truth_mask, idx, iou = data\n",
    "    image_np = image.cpu().numpy().transpose(1, 2, 0)  \n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.suptitle(f\"{title} (IoU: {iou:.4f})\")\n",
    "\n",
    "    # Display original image\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(image_np)\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Display ground truth mask\n",
    "    plt.subplot(1, 3, 2)\n",
    "    gt_mask_np = ground_truth_mask.squeeze().cpu().numpy() \n",
    "    plt.imshow(gt_mask_np, cmap=\"gray\")\n",
    "    plt.title(\"Ground Truth Mask\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Display predicted mask\n",
    "    plt.subplot(1, 3, 3)\n",
    "    pred_mask_np = prediction_mask.squeeze().cpu().numpy()\n",
    "    plt.imshow(pred_mask_np, cmap=\"gray\")\n",
    "    plt.title(\"Predicted Mask\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "test_images_dir = \"../data/model_training/split/test/images\"\n",
    "test_masks_dir = \"../data/model_training/split/test/masks\"\n",
    "test_dataset = SegmentationDataset(\n",
    "    test_images_dir, test_masks_dir, transform=val_transform\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "find_best_and_worst_predictions_unet(model, test_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
