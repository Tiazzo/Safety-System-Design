{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import utils\n",
    "import cv2\n",
    "import os \n",
    "import sys\n",
    "import matplotlib.pyplot as plt \n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from albumentations import Resize, Compose, Normalize, HorizontalFlip, RandomBrightnessContrast\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "\n",
    "# import functions\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from src.utils import *\n",
    "from src.dataset_manager import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = '../data/with_labels/images'\n",
    "labels_path = '../data/with_labels/json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if every image has a json file with the same name\n",
    "check_images_labels(images_path=images_path, labels_path=labels_path)\n",
    "\n",
    "# Number of images\n",
    "n_images = get_num_images(images_path=images_path)\n",
    "n_labels = get_num_labels(labels_path=labels_path)\n",
    "print(f\"Number of images: {n_images}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flip images to the same direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the YOLO  model\n",
    "yolo_model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "orientation_model = models.resnet18(pretrained=True)\n",
    "orientation_model.fc = torch.nn.Linear(orientation_model.fc.in_features, 2)\n",
    "orientation_model = orientation_model.eval()\n",
    "\n",
    "# Define image preprocessing for the orientation classifier\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bounding_box(image_path, bounding_boxes):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    img_height, img_width = image.shape[:2]\n",
    "\n",
    "    # Define the colors\n",
    "    colors = {\n",
    "        0: (255, 0, 0),\n",
    "        1: (0, 255, 0),\n",
    "        2: (0, 0, 255),\n",
    "        3: (255, 255, 0),\n",
    "        4: (255, 0, 255),\n",
    "        5: (0, 255, 255),\n",
    "    }\n",
    "\n",
    "    for bbox in bounding_boxes:\n",
    "        class_id, x_center, y_center, width, height = bbox\n",
    "\n",
    "        # Convert the coordinates to absolute values\n",
    "        x_center, y_center = int(x_center * img_width), int(y_center * img_height)\n",
    "        width, height = int(width * img_width), int(height * img_height)\n",
    "\n",
    "        # Calculate the top-left and bottom-right corner of the bounding box\n",
    "        x1 = int(x_center - width / 2)\n",
    "        y1 = int(y_center - height / 2)\n",
    "        x2 = int(x_center + width / 2)\n",
    "        y2 = int(y_center + height / 2)\n",
    "\n",
    "        # Class color\n",
    "        color = colors[class_id]\n",
    "\n",
    "        # Draw the bounding box\n",
    "        image = cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)\n",
    "        # Add the class name\n",
    "        cv2.putText(\n",
    "            image, str(class_id), (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2\n",
    "        )\n",
    "\n",
    "    # Display the image\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = '../data/processed/images/A_Ast_01.jpg'\n",
    "image_label_path = '../data/processed/labels/A_Ast_01.txt'\n",
    "\n",
    "bounding_boxes = []\n",
    "with open(image_label_path, 'r') as file:\n",
    "    for line in file:\n",
    "        class_id, x_center, y_center, width, height = line.strip().split()\n",
    "        bounding_boxes.append([int(class_id), float(x_center), float(y_center), float(width), float(height)])\n",
    "\n",
    "draw_bounding_box(image_path, bounding_boxes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert LabelME JSON labels into COCO formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelme_dir = \"../data/with_labels/json_flipped\"\n",
    "coco_labels_file = \"../data/with_labels/labels/coco_annotations.json\"\n",
    "labelme_to_coco(labelme_dir, coco_labels_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization COCO Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "def visualize_coco_annotations(coco_json, images_dir, num_samples=5):\n",
    "    \"\"\"\n",
    "    Visualizes COCO annotations by overlaying bounding boxes and segmentation masks on images.\n",
    "    \n",
    "    Parameters:\n",
    "        coco_json (str): Path to the COCO JSON file.\n",
    "        images_dir (str): Directory containing the images.\n",
    "        num_samples (int): Number of images to visualize.\n",
    "    \"\"\"\n",
    "    # Load COCO JSON\n",
    "    with open(coco_json, 'r') as f:\n",
    "        coco_data = json.load(f)\n",
    "\n",
    "    # Map image IDs to image metadata\n",
    "    image_dict = {img[\"id\"]: img for img in coco_data[\"images\"]}\n",
    "\n",
    "    # Map annotations by image_id\n",
    "    annotations_by_image = {}\n",
    "    for ann in coco_data[\"annotations\"]:\n",
    "        image_id = ann[\"image_id\"]\n",
    "        if image_id not in annotations_by_image:\n",
    "            annotations_by_image[image_id] = []\n",
    "        annotations_by_image[image_id].append(ann)\n",
    "\n",
    "    # Visualize a subset of images\n",
    "    for img_id, img_data in list(image_dict.items())[:num_samples]:\n",
    "        img_path = Path(images_dir) / img_data[\"file_name\"]\n",
    "        image = cv2.imread(str(img_path))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB for Matplotlib\n",
    "\n",
    "        # Overlay annotations\n",
    "        if img_id in annotations_by_image:\n",
    "            for ann in annotations_by_image[img_id]:\n",
    "                # Draw bounding box\n",
    "                bbox = ann[\"bbox\"]\n",
    "                x_min, y_min, width, height = map(int, bbox)\n",
    "                x_max, y_max = x_min + width, y_min + height\n",
    "                cv2.rectangle(image, (x_min, y_min), (x_max, y_max), color=(0, 255, 0), thickness=2)\n",
    "\n",
    "                # Draw segmentation (if available)\n",
    "                if \"segmentation\" in ann:\n",
    "                    points = ann[\"segmentation\"][0]\n",
    "                    points = [(int(points[i]), int(points[i + 1])) for i in range(0, len(points), 2)]\n",
    "                    for i in range(len(points)):\n",
    "                        cv2.line(image, points[i], points[(i + 1) % len(points)], color=(255, 0, 0), thickness=2)\n",
    "\n",
    "        # Display the image with annotations\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(image)\n",
    "        plt.title(f\"Image ID: {img_id}\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "# Example usage\n",
    "images_dir = \"../data/with_labels/images_flipped\"        # Path to your image directory\n",
    "visualize_coco_annotations(coco_labels_file, images_dir, num_samples=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the file name of the masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_clean = '../data/with_labels/json_flipped'\n",
    "clean_image_path(path_to_clean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotate images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correct_image_rotation('../data/with_labels/images_flipped', '../data/with_labels/images_tilted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert LabelMe labels to mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dir = \"../data/with_labels/images_flipped\"  # Path to normal images\n",
    "labelme_dir = \"../data/with_labels/json_flipped\"  # Directory containing LabelMe JSON files\n",
    "output_mask_dir = \"../data/with_labels/masks\"  # Directory of masks\n",
    "output_augmentations_dir = \"../data/with_labels/augmented\"  # Directory to save augmented images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_masks(labelme_dir, output_mask_dir):\n",
    "    \"\"\"\n",
    "    Converts LabelMe JSON annotations to binary masks.\n",
    "\n",
    "    Parameters:\n",
    "        labelme_dir (str): Directory containing LabelMe JSON files.\n",
    "        output_mask_dir (str): Directory to save generated masks.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_mask_dir, exist_ok=True)\n",
    "\n",
    "    for json_file in Path(labelme_dir).glob(\"*.json\"):\n",
    "        with open(json_file, \"r\") as f:\n",
    "            label_data = json.load(f)\n",
    "\n",
    "        # Get image dimensions\n",
    "        img_height = label_data[\"imageHeight\"]\n",
    "        img_width = label_data[\"imageWidth\"]\n",
    "\n",
    "        # Create an empty mask\n",
    "        mask = np.zeros((img_height, img_width), dtype=np.uint8)\n",
    "\n",
    "        # Add polygons to the mask\n",
    "        for shape in label_data[\"shapes\"]:\n",
    "            if shape[\"label\"] == \"window\":  # Use only \"window\" labels\n",
    "                points = np.array(shape[\"points\"], dtype=np.int32)\n",
    "                cv2.fillPoly(mask, [points], color=1)\n",
    "\n",
    "        # Save the mask\n",
    "        mask_name = Path(label_data[\"imagePath\"]).stem + \".png\"\n",
    "        mask_path = os.path.join(output_mask_dir, mask_name)\n",
    "        cv2.imwrite(mask_path, mask * 255)  # Convert to 0-255 scale for saving as PNG\n",
    "\n",
    "    print(f\"Masks saved\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_masks(labelme_dir, output_mask_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masks visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_mask(image_path, mask_path):\n",
    "    \"\"\"\n",
    "    Visualizes the original image and its corresponding mask.\n",
    "\n",
    "    Parameters:\n",
    "        image_path (str): Path to the input image.\n",
    "        mask_path (str): Path to the binary mask.\n",
    "    \"\"\"\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image)\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(mask, cmap=\"gray\")\n",
    "    plt.title(\"Segmentation Mask\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_mask(\"../data/with_labels/images_flipped/A_Fia_01.jpg\", \"../data/with_labels/masks/A_Fia_01.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the transformations for segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_dataset(images_dir, masks_dir, output_dir, num_augmented=2):\n",
    "    \"\"\"\n",
    "    Augments an entire dataset of images and corresponding masks for segmentation tasks.\n",
    "\n",
    "    Parameters:\n",
    "        images_dir (str): Directory containing input images.\n",
    "        masks_dir (str): Directory containing input masks.\n",
    "        output_dir (str): Directory to save augmented images and masks.\n",
    "        num_augmented (int): Number of augmented copies to generate for each image-mask pair.\n",
    "    \"\"\"\n",
    "    transform = A.Compose(\n",
    "        [\n",
    "            #A.HorizontalFlip(p=0.5),  # Flips image and mask horizontally.\n",
    "            A.RandomBrightnessContrast(p=0.2),  # Adjust brightness and contrast of the image.\n",
    "            # A.ShiftScaleRotate(\n",
    "            #     shift_limit=0.05,\n",
    "            #     scale_limit=0.05,\n",
    "            #     rotate_limit=15,\n",
    "            #     border_mode=0,  # Padding ensures dimensions remain consistent.\n",
    "            #     p=0.5,\n",
    "            # ),\n",
    "            A.GaussianBlur(blur_limit=(3, 5), p=0.2),  # Adds slight blurring.\n",
    "            A.HueSaturationValue(\n",
    "                hue_shift_limit=10, sat_shift_limit=20, val_shift_limit=10, p=0.3\n",
    "            ),  # Changes hue, saturation, and value.\n",
    "            A.Resize(1024, 1024, always_apply=True),  # Ensures image and mask are resized consistently.\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Create output directories\n",
    "    output_images_dir = Path(output_dir) / \"images\"\n",
    "    output_masks_dir = Path(output_dir) / \"masks\"\n",
    "    output_images_dir.mkdir(parents=True, exist_ok=True)\n",
    "    output_masks_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Process all images and masks\n",
    "    for image_path in Path(images_dir).glob(\"*.jpg\"):\n",
    "        mask_path = Path(masks_dir) / f\"{image_path.stem}.png\"\n",
    "        if not mask_path.exists():\n",
    "            print(f\"Mask for {image_path.name} not found. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Load image and mask\n",
    "        image = cv2.imread(str(image_path))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        for i in range(num_augmented):\n",
    "            # Apply augmentation\n",
    "            transformed = transform(image=image, mask=mask)\n",
    "            aug_image = transformed[\"image\"]\n",
    "            aug_mask = transformed[\"mask\"]\n",
    "\n",
    "            # Save augmented image and mask\n",
    "            aug_image_name = f\"{image_path.stem}_aug_{i}.jpg\"\n",
    "            aug_mask_name = f\"{mask_path.stem}_aug_{i}.png\"\n",
    "            cv2.imwrite(\n",
    "                str(output_images_dir / aug_image_name),\n",
    "                cv2.cvtColor(aug_image, cv2.COLOR_RGB2BGR),\n",
    "            )\n",
    "            cv2.imwrite(str(output_masks_dir / aug_mask_name), aug_mask)\n",
    "\n",
    "    print(f\"Augmented dataset saved in {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_dataset(images_dir, output_mask_dir, output_augmentations_dir, num_augmented=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize augmented images and masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_augmented_data(image_dir, mask_dir, num_samples=5):\n",
    "    \"\"\"\n",
    "    Visualizes a few augmented image-mask pairs.\n",
    "\n",
    "    Parameters:\n",
    "        image_dir (str): Directory containing augmented images.\n",
    "        mask_dir (str): Directory containing augmented masks.\n",
    "        num_samples (int): Number of samples to visualize.\n",
    "    \"\"\"\n",
    "    image_paths = sorted(Path(image_dir).glob(\"*.jpg\"))[:num_samples]\n",
    "    mask_paths = sorted(Path(mask_dir).glob(\"*.png\"))[:num_samples]\n",
    "\n",
    "    for img_path, mask_path in zip(image_paths, mask_paths):\n",
    "        image = cv2.imread(str(img_path))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(image)\n",
    "        plt.title(f\"Augmented Image: {img_path.name}\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(mask, cmap=\"gray\")\n",
    "        plt.title(f\"Augmented Mask: {mask_path.name}\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_images_dir = \"../data/with_labels/augmented/images\"\n",
    "augmented_masks_dir = \"../data/with_labels/augmented/masks\"\n",
    "visualize_augmented_data(augmented_images_dir, augmented_masks_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split of dateset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "augmented_images_dir = \"../data/with_labels/augmented/images\"\n",
    "augmented_masks_dir = \"../data/with_labels/augmented/masks\"\n",
    "output_split_dir = \"../data/split\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_augmented_dataset(augmented_images_dir, augmented_masks_dir, output_split_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_split(split_dir, num_samples=1):\n",
    "    images_dir = Path(split_dir) / \"images\"\n",
    "    masks_dir = Path(split_dir) / \"masks\"\n",
    "\n",
    "    image_paths = sorted(images_dir.glob(\"*.jpg\"))[:num_samples]\n",
    "    mask_paths = sorted(masks_dir.glob(\"*.png\"))[:num_samples]\n",
    "\n",
    "    for img_path, mask_path in zip(image_paths, mask_paths):\n",
    "        image = cv2.imread(str(img_path))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(image)\n",
    "        plt.title(f\"Image: {img_path.name}\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(mask, cmap=\"gray\")\n",
    "        plt.title(f\"Mask: {mask_path.name}\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_split(\"../data/split/train\")\n",
    "visualize_split(\"../data/split/val\")\n",
    "visualize_split(\"../data/split/test\")\n",
    "\n",
    "# Print the number of images in each split\n",
    "train_images = get_num_images(images_path=\"../data/split/train/images\")\n",
    "val_images = get_num_images(images_path=\"../data/split/val/images\")\n",
    "test_images = get_num_images(images_path=\"../data/split/test/images\")\n",
    "\n",
    "print(f\"Number of training images: {train_images}\")\n",
    "print(f\"Number of validation images: {val_images}\")\n",
    "print(f\"Number of testing images: {test_images}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, images_dir, masks_dir, transform=None):\n",
    "        self.images_dir = images_dir\n",
    "        self.masks_dir = masks_dir\n",
    "        self.image_paths = sorted(list(Path(images_dir).glob(\"*.jpg\")))\n",
    "        self.mask_paths = sorted(list(Path(masks_dir).glob(\"*.png\")))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        mask_path = self.mask_paths[idx]\n",
    "\n",
    "        # Load image and mask\n",
    "        image = cv2.imread(str(image_path))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented[\"image\"]\n",
    "            mask = augmented[\"mask\"]\n",
    "\n",
    "        # Normalize mask (binary: 0 or 1)\n",
    "        mask = torch.tensor(mask, dtype=torch.float32).unsqueeze(0) / 255.0\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentations during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training augmentations\n",
    "train_transform = Compose(\n",
    "    [\n",
    "        HorizontalFlip(p=0.5),\n",
    "        RandomBrightnessContrast(p=0.2),\n",
    "        Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        Resize(512, 512),  # Resize to 512x512 (adjust based on model input size)\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Validation augmentations\n",
    "val_transform = Compose(\n",
    "    [\n",
    "        Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        Resize(512, 512),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_dir = \"../data/split/train/images\"\n",
    "train_masks_dir = \"../data/split/train/masks\"\n",
    "val_images_dir = \"../data/split/val/images\"\n",
    "val_masks_dir = \"../data/split/val/masks\"\n",
    "\n",
    "train_dataset = SegmentationDataset(train_images_dir, train_masks_dir, transform=train_transform)\n",
    "val_dataset = SegmentationDataset(val_images_dir, val_masks_dir, transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install segmentation_models_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segmentation_models_pytorch import Unet\n",
    "\n",
    "# Define the U-Net model\n",
    "model = Unet(\n",
    "    encoder_name=\"resnet34\",  # Encoder backbone\n",
    "    encoder_weights=\"imagenet\",  # Pretrained on ImageNet\n",
    "    in_channels=3,  # Input channels (RGB)\n",
    "    classes=1,  # Output channels (binary segmentation)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "\n",
    "# Device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = BCEWithLogitsLoss()  # Combines sigmoid activation and binary cross-entropy\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, path=\"checkpoint.pth\"):\n",
    "    checkpoint = {\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"epoch\": epoch,\n",
    "    }\n",
    "    torch.save(checkpoint, path)\n",
    "    print(f\"Checkpoint saved at epoch {epoch} to {path}\")\n",
    "\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for images, masks in train_loader:\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {train_loss:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, masks in val_loader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Save the best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "        print(\"Best model saved!\")\n",
    "\n",
    "    # Save periodic checkpoints\n",
    "    if epoch % 5 == 0:\n",
    "        save_checkpoint(model, optimizer, epoch, path=f\"checkpoint_epoch_{epoch}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(model, image, mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = model(image.unsqueeze(0).to(device))\n",
    "        pred = torch.sigmoid(pred).squeeze().cpu().numpy() > 0.5\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(image.permute(1, 2, 0).cpu().numpy())\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(mask.squeeze().cpu().numpy(), cmap=\"gray\")\n",
    "    plt.title(\"True Mask\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(pred, cmap=\"gray\")\n",
    "    plt.title(\"Predicted Mask\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Visualize on a validation sample\n",
    "sample_image, sample_mask = next(iter(val_loader))\n",
    "visualize_predictions(model, sample_image[7], sample_mask[7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from segmentation_models_pytorch import Unet\n",
    "\n",
    "# Define the U-Net model with the same configuration as during training\n",
    "model = Unet(\n",
    "    encoder_name=\"resnet34\",  # Encoder backbone\n",
    "    encoder_weights=None,    # No pretraining needed when loading weights\n",
    "    in_channels=3,           # Input channels (RGB)\n",
    "    classes=1,               # Output channels (binary segmentation)\n",
    ")\n",
    "\n",
    "# Load the model weights\n",
    "model_path = \"../checkpoints/best_model.pth\"  # Path to the saved model\n",
    "state_dict = torch.load(model_path, map_location=torch.device(\"cpu\"))  # Load weights\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate performance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yushe\\AppData\\Local\\Temp\\ipykernel_22368\\2161048088.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mask = torch.tensor(mask, dtype=torch.float32).unsqueeze(0) / 255.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean IoU: 0.7429\n",
      "Mean Dice: 0.8439\n",
      "Mean Precision: 0.9664\n",
      "Mean Recall: 0.7579\n",
      "Mean F1-Score: 0.8439\n",
      "Mean MAE: 0.0062\n",
      "Mean Hausdorff Distance: 28.2093\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from scipy.spatial.distance import directed_hausdorff\n",
    "\n",
    "def compute_metrics(model, dataloader):\n",
    "    model.eval()\n",
    "    iou_scores = []\n",
    "    dice_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "    hausdorff_distances = []\n",
    "    mae_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks in dataloader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            outputs = model(images)\n",
    "            preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "\n",
    "            # Compute metrics for each sample\n",
    "            for pred, mask in zip(preds, masks):\n",
    "                intersection = (pred * mask).sum().item()\n",
    "                pred_sum = pred.sum().item()\n",
    "                mask_sum = mask.sum().item()\n",
    "                union = pred_sum + mask_sum - intersection\n",
    "\n",
    "                # IoU\n",
    "                iou = intersection / (union + 1e-6)\n",
    "                iou_scores.append(iou)\n",
    "\n",
    "                # Dice\n",
    "                dice = 2 * intersection / (pred_sum + mask_sum + 1e-6)\n",
    "                dice_scores.append(dice)\n",
    "\n",
    "                # Precision and Recall\n",
    "                tp = intersection\n",
    "                fp = pred_sum - intersection\n",
    "                fn = mask_sum - intersection\n",
    "                precision = tp / (tp + fp + 1e-6)\n",
    "                recall = tp / (tp + fn + 1e-6)\n",
    "                precision_scores.append(precision)\n",
    "                recall_scores.append(recall)\n",
    "\n",
    "                # F1 Score\n",
    "                f1 = 2 * precision * recall / (precision + recall + 1e-6)\n",
    "                f1_scores.append(f1)\n",
    "\n",
    "                 # Mean Absolute Error (MAE)\n",
    "                mae = torch.abs(pred - mask).mean().item()\n",
    "                mae_scores.append(mae)\n",
    "\n",
    "                # Hausdorff Distance\n",
    "                pred_indices = torch.nonzero(pred, as_tuple=False).cpu().numpy()\n",
    "                mask_indices = torch.nonzero(mask, as_tuple=False).cpu().numpy()\n",
    "                if len(pred_indices) > 0 and len(mask_indices) > 0:\n",
    "                    hd = max(\n",
    "                        directed_hausdorff(pred_indices, mask_indices)[0],\n",
    "                        directed_hausdorff(mask_indices, pred_indices)[0],\n",
    "                    )\n",
    "                else:\n",
    "                    hd = float(\"inf\")  # Handle case with empty prediction or mask\n",
    "                hausdorff_distances.append(hd)\n",
    "\n",
    "    \n",
    "\n",
    "    # Calculate mean metrics\n",
    "    mean_iou = np.mean(iou_scores)\n",
    "    mean_dice = np.mean(dice_scores)\n",
    "    mean_precision = np.mean(precision_scores)\n",
    "    mean_recall = np.mean(recall_scores)\n",
    "    mean_f1 = np.mean(f1_scores)\n",
    "    mean_mae = np.mean(mae_scores)\n",
    "    #mean_hausdorff = np.mean(hausdorff_distances)\n",
    "    mean_hausdorff = np.mean([d for d in hausdorff_distances if d != float(\"inf\")])\n",
    "\n",
    "\n",
    "    print(f\"Mean IoU: {mean_iou:.4f}\")\n",
    "    print(f\"Mean Dice: {mean_dice:.4f}\")\n",
    "    print(f\"Mean Precision: {mean_precision:.4f}\")\n",
    "    print(f\"Mean Recall: {mean_recall:.4f}\")\n",
    "    print(f\"Mean F1-Score: {mean_f1:.4f}\")\n",
    "    print(f\"Mean MAE: {mean_mae:.4f}\")\n",
    "    print(f\"Mean Hausdorff Distance: {mean_hausdorff:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"iou\": mean_iou,\n",
    "        \"dice\": mean_dice,\n",
    "        \"precision\": mean_precision,\n",
    "        \"recall\": mean_recall,\n",
    "        \"f1\": mean_f1,\n",
    "        \"mae\": mean_mae,\n",
    "        \"hausdorff\": mean_hausdorff,\n",
    "    }\n",
    "\n",
    "device='cpu'\n",
    "# Evaluate on the test set\n",
    "test_images_dir = \"../data/split/test/images\"\n",
    "test_masks_dir = \"../data/split/test/masks\"\n",
    "test_dataset = SegmentationDataset(\n",
    "    test_images_dir, test_masks_dir, transform=val_transform\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "# Evaluate on the test set\n",
    "metrics = compute_metrics(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(model, dataloader):\n",
    "    model.eval()\n",
    "    iou_scores = []\n",
    "    dice_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks in dataloader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            outputs = model(images)\n",
    "            preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "\n",
    "            # Compute IoU and Dice for each sample\n",
    "            for pred, mask in zip(preds, masks):\n",
    "                intersection = (pred * mask).sum()\n",
    "                union = pred.sum() + mask.sum() - intersection\n",
    "                iou = intersection / (union + 1e-6)\n",
    "                dice = 2 * intersection / (pred.sum() + mask.sum() + 1e-6)\n",
    "\n",
    "                iou_scores.append(iou.item())\n",
    "                dice_scores.append(dice.item())\n",
    "\n",
    "    mean_iou = np.mean(iou_scores)\n",
    "    mean_dice = np.mean(dice_scores)\n",
    "    print(f\"Mean IoU: {mean_iou:.4f}, Mean Dice: {mean_dice:.4f}\")\n",
    "\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_images_dir = \"/kaggle/input/cars-side/split/test/images\"\n",
    "test_masks_dir = \"/kaggle/input/cars-side/split/test/masks\"\n",
    "test_dataset = SegmentationDataset(\n",
    "    test_images_dir, test_masks_dir, transform=val_transform\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "compute_metrics(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from scipy.spatial.distance import directed_hausdorff\n",
    "\n",
    "def compute_metrics(model, dataloader):\n",
    "    model.eval()\n",
    "    iou_scores = []\n",
    "    dice_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "    hausdorff_distances = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks in dataloader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            outputs = model(images)\n",
    "            preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "\n",
    "            # Compute metrics for each sample\n",
    "            for pred, mask in zip(preds, masks):\n",
    "                intersection = (pred * mask).sum().item()\n",
    "                pred_sum = pred.sum().item()\n",
    "                mask_sum = mask.sum().item()\n",
    "                union = pred_sum + mask_sum - intersection\n",
    "\n",
    "                # IoU\n",
    "                iou = intersection / (union + 1e-6)\n",
    "                iou_scores.append(iou)\n",
    "\n",
    "                # Dice\n",
    "                dice = 2 * intersection / (pred_sum + mask_sum + 1e-6)\n",
    "                dice_scores.append(dice)\n",
    "\n",
    "                # Precision and Recall\n",
    "                tp = intersection\n",
    "                fp = pred_sum - intersection\n",
    "                fn = mask_sum - intersection\n",
    "                precision = tp / (tp + fp + 1e-6)\n",
    "                recall = tp / (tp + fn + 1e-6)\n",
    "                precision_scores.append(precision)\n",
    "                recall_scores.append(recall)\n",
    "\n",
    "                # F1 Score\n",
    "                f1 = 2 * precision * recall / (precision + recall + 1e-6)\n",
    "                f1_scores.append(f1)\n",
    "\n",
    "                # Hausdorff Distance\n",
    "                pred_indices = torch.nonzero(pred, as_tuple=False).cpu().numpy()\n",
    "                mask_indices = torch.nonzero(mask, as_tuple=False).cpu().numpy()\n",
    "                if len(pred_indices) > 0 and len(mask_indices) > 0:\n",
    "                    hd = max(\n",
    "                        directed_hausdorff(pred_indices, mask_indices)[0],\n",
    "                        directed_hausdorff(mask_indices, pred_indices)[0],\n",
    "                    )\n",
    "                else:\n",
    "                    hd = float(\"inf\")  # Handle case with empty prediction or mask\n",
    "                hausdorff_distances.append(hd)\n",
    "\n",
    "    # Calculate mean metrics\n",
    "    mean_iou = np.mean(iou_scores)\n",
    "    mean_dice = np.mean(dice_scores)\n",
    "    mean_precision = np.mean(precision_scores)\n",
    "    mean_recall = np.mean(recall_scores)\n",
    "    mean_f1 = np.mean(f1_scores)\n",
    "    mean_hausdorff = np.mean(hausdorff_distances)\n",
    "\n",
    "    print(f\"Mean IoU: {mean_iou:.4f}\")\n",
    "    print(f\"Mean Dice: {mean_dice:.4f}\")\n",
    "    print(f\"Mean Precision: {mean_precision:.4f}\")\n",
    "    print(f\"Mean Recall: {mean_recall:.4f}\")\n",
    "    print(f\"Mean F1-Score: {mean_f1:.4f}\")\n",
    "    print(f\"Mean Hausdorff Distance: {mean_hausdorff:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"iou\": mean_iou,\n",
    "        \"dice\": mean_dice,\n",
    "        \"precision\": mean_precision,\n",
    "        \"recall\": mean_recall,\n",
    "        \"f1\": mean_f1,\n",
    "        \"hausdorff\": mean_hausdorff,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def visualize_test_results(model, dataloader, num_samples=10):\n",
    "    \"\"\"\n",
    "    Visualizes random test results by displaying original images, true masks, and predicted masks.\n",
    "\n",
    "    Parameters:\n",
    "        model (torch.nn.Module): Trained model for inference.\n",
    "        dataloader (DataLoader): DataLoader containing test dataset.\n",
    "        num_samples (int): Number of random samples to visualize.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Flatten the dataset into a list of indices\n",
    "    dataset_indices = list(range(len(dataloader.dataset)))\n",
    "\n",
    "    # Randomly sample indices\n",
    "    selected_indices = random.sample(dataset_indices, min(num_samples, len(dataset_indices)))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx in selected_indices:\n",
    "            # Retrieve the image and mask at the selected index\n",
    "            image, mask = dataloader.dataset[idx]\n",
    "\n",
    "            # Convert to device and add batch dimension\n",
    "            image_tensor = image.unsqueeze(0).to(device)\n",
    "            mask_tensor = mask.unsqueeze(0).to(device)\n",
    "\n",
    "            # Model prediction\n",
    "            output = model(image_tensor)\n",
    "            pred = (torch.sigmoid(output) > 0.5).float()\n",
    "\n",
    "            # Visualize\n",
    "            plt.figure(figsize=(12, 6))\n",
    "\n",
    "            # Original image\n",
    "            plt.subplot(1, 3, 1)\n",
    "            plt.imshow(image.permute(1, 2, 0).cpu().numpy())\n",
    "            plt.title(\"Original Image\")\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "            # True mask\n",
    "            plt.subplot(1, 3, 2)\n",
    "            plt.imshow(mask.squeeze().cpu().numpy(), cmap=\"gray\")\n",
    "            plt.title(\"True Mask\")\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "            # Predicted mask\n",
    "            plt.subplot(1, 3, 3)\n",
    "            plt.imshow(pred.squeeze().cpu().numpy(), cmap=\"gray\")\n",
    "            plt.title(\"Predicted Mask\")\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segmentation_models_pytorch import Unet\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = Unet(encoder_name=\"resnet34\", in_channels=3, classes=1).to(device)\n",
    "model.load_state_dict(torch.load(\"../checkpoints/best_model.pth\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "test_images_dir = \"../data/split/test/images\"\n",
    "test_masks_dir = \"../data/split/test/masks\"\n",
    "\n",
    "test_transform = Compose([\n",
    "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    Resize(512, 512),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "test_dataset = SegmentationDataset(test_images_dir, test_masks_dir, transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# 3. Visualize predictions\n",
    "for images, masks in test_loader:\n",
    "    images, masks = images.to(device), masks.to(device)\n",
    "    outputs = model(images)\n",
    "    preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "    \n",
    "    visualize_test_results(model, test_loader, num_samples=10)\n",
    "    break "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
